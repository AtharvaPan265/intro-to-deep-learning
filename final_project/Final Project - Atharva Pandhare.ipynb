{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf03dff",
   "metadata": {},
   "source": [
    "# LSTM Cell Writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2e97f3",
   "metadata": {},
   "source": [
    "# LSTM Cell Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3136cbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Data directory: data/Car_data/car_data\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import os\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data directory\n",
    "csv_dir = \"data/Car_data/car_data\"\n",
    "print(f\"Data directory: {csv_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fb5047f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 9400\n",
      "Training files: 7520\n",
      "Testing files: 1880\n",
      "Verification: True\n"
     ]
    }
   ],
   "source": [
    "# List all CSV files\n",
    "csv_files = [os.path.join(csv_dir, f)\n",
    "             for f in os.listdir(csv_dir) if f.endswith('.csv')]\n",
    "\n",
    "# List all CSV files\n",
    "\n",
    "print(f\"Total files: {len(csv_files)}\")\n",
    "\n",
    "# Split into train/test sets\n",
    "data_80 = int(len(csv_files) * 0.8)\n",
    "trainset = csv_files[:data_80]\n",
    "testset = csv_files[data_80:]\n",
    "print(f\"Training files: {len(trainset)}\")\n",
    "print(f\"Testing files: {len(testset)}\")\n",
    "print(f\"Verification: {(len(trainset) + len(testset)) == len(csv_files)}\")\n",
    "\n",
    "# Fit scaler on all training data\n",
    "all_train_data = []\n",
    "for file_path in trainset:\n",
    "    df = pd.read_csv(file_path)\n",
    "    all_train_data.append(df.values)\n",
    "\n",
    "all_train_data = np.vstack(all_train_data)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(all_train_data)\n",
    "\n",
    "# Lists to store sequences\n",
    "all_sequences = []\n",
    "\n",
    "# Process each file\n",
    "for file_path in trainset:\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Scale the data\n",
    "    scaled_data = scaler.transform(df.values)\n",
    "\n",
    "    # Store the sequence\n",
    "    all_sequences.append(scaled_data)\n",
    "\n",
    "# Create input/output sequences\n",
    "X = []  # Input: 62 time steps, all 12 features\n",
    "y = []  # Output: next 5 time steps, only x,y coordinates\n",
    "\n",
    "for sequence in all_sequences:\n",
    "    # Only use sequences with enough time steps\n",
    "    if len(sequence) >= 67:\n",
    "        for i in range(len(sequence) - 67 + 1):\n",
    "            # 62 time steps as input, all features\n",
    "            X.append(sequence[i:i+62, :])\n",
    "            # next 5 time steps, only x,y coordinates\n",
    "            y.append(sequence[i+62:i+67, :2])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(np.array(X), dtype=torch.float32)\n",
    "y = torch.tensor(np.array(y), dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(X, y)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                          shuffle=False, pin_memory=True, num_workers=12\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2853532",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_custom(nn.Module):\n",
    "    def __init__(self, input_size=12, hidden_size=64, num_layers=1, sequence_length=10, dropout=0.2):\n",
    "\n",
    "        super(LSTM_custom, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Input gate\n",
    "        self.W_xi = nn.Linear(input_size, hidden_size)\n",
    "        self.W_hi = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # Forget gate\n",
    "        self.W_xf = nn.Linear(input_size, hidden_size)\n",
    "        self.W_hf = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # Cell gate\n",
    "        self.W_xg = nn.Linear(input_size, hidden_size)\n",
    "        self.W_hg = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # Output gate\n",
    "        self.W_xo = nn.Linear(input_size, hidden_size)\n",
    "        self.W_ho = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def lstm_cell(self, x_t, h_prev, c_prev):\n",
    "        i_t = torch.sigmoid(self.W_xi(x_t) + self.W_hi(h_prev))\n",
    "        f_t = torch.sigmoid(self.W_xf(x_t) + self.W_hf(h_prev))\n",
    "        g_t = torch.tanh(self.W_xg(x_t) + self.W_hg(h_prev))\n",
    "        o_t = torch.sigmoid(self.W_xo(x_t) + self.W_ho(h_prev))\n",
    "\n",
    "        c_t = f_t * c_prev + i_t * g_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "        return h_t, c_t\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        h_t = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "        c_t = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "        \n",
    "        # Process the input sequence once\n",
    "        for t in range(seq_len):\n",
    "            h_t, c_t = self.lstm_cell(x[:, t, :], h_t, c_t)\n",
    "        \n",
    "        # Predict next 5 timesteps\n",
    "        predictions = []\n",
    "        current_x = x[:, -1, :]  # Start with the last input\n",
    "        \n",
    "        for i in range(5):\n",
    "            h_t, c_t = self.lstm_cell(current_x, h_t, c_t)\n",
    "            predictions.append(h_t)\n",
    "            # You would need a projection layer to convert h_t to the right dimension\n",
    "            # current_x = projection_layer(h_t)\n",
    "        \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean((y_pred - y_true) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476d37ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_size = 12  # Number of input features (still use all features as input)\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "model = LSTM_custom(input_size=input_size, hidden_size=hidden_size,\n",
    "                    num_layers=num_layers).to(device)\n",
    "# Loss and optimizer\n",
    "\n",
    "criterion = rmse_loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c60ab94",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'targets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtargets\u001b[49m.shape)\n",
      "\u001b[31mNameError\u001b[39m: name 'targets' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caa594c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 62, 12])\n",
      "torch.Size([32, 5, 2])\n",
      "torch.Size([32, 62, 12])\n",
      "torch.Size([32, 5, 2])\n",
      "torch.Size([32, 62, 12])\n",
      "torch.Size([32, 5, 2])\n",
      "torch.Size([32, 62, 12])\n",
      "torch.Size([32, 5, 2])\n",
      "torch.Size([32, 62, 12])\n",
      "torch.Size([32, 5, 2])\n",
      "torch.Size([32, 62, 12])\n",
      "torch.Size([32, 5, 2])\n",
      "torch.Size([32, 62, 12])\n",
      "torch.Size([32, 5, 2])\n",
      "torch.Size([32, 62, 12])\n",
      "torch.Size([32, 5, 2])\n",
      "torch.Size([32, 62, 12])\n",
      "torch.Size([32, 5, 2])\n",
      "torch.Size([32, 62, 12])\n",
      "torch.Size([32, 5, 2])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m     optimizer.step()\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (epoch + \u001b[32m1\u001b[39m) % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        print(inputs.shape)\n",
    "        print(targets.shape)\n",
    "        break\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs[-1], targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
