{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf03dff",
   "metadata": {},
   "source": [
    "# LSTM Cell Writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce89427",
   "metadata": {},
   "source": [
    "\n",
    "## Input Gate($i$)\n",
    "### I/O\n",
    "\n",
    "$x_t = 1 \\times I_{size}$\n",
    "\n",
    "$W_{x_i} = H_{size} \\times 1$\n",
    "\n",
    "$W_{X_i}x_t = H_{size} \\times I_{size}$\n",
    "\n",
    "### Math\n",
    "\n",
    "$$i_t = \\sigma(W_{x_i}x_t+W_{h_i}h_{t-1}+b_i))$$\n",
    "- Partials\n",
    "\t- $i_t = \\sigma (p)$\n",
    "\t\t- $\\nabla_pi_t=(1-\\sigma(p))\\sigma(p)$\n",
    "\t- $p = q + v + b_i$\n",
    "\t\t- $\\nabla_qp=1$\n",
    "\t\t- $\\nabla_vp=1$\n",
    "\t\t- $\\nabla_{b_i}p=1$\n",
    "\t- $q = W_{x_i}x_t$\n",
    "\t\t- $\\nabla_{W_{x_i}}q = x_t^T$\n",
    "\t\t- $\\nabla_{x_t}q = W_{x_i}^T$\n",
    "\t- $v = W_{h_i}h_{t-1}$\n",
    "\t\t- $\\nabla_{W_{h_i}}q = h_{t-1}^T$\n",
    "\t\t- $\\nabla_{h_{t-1}}q = W_{h_i}^T$\n",
    "- $\\nabla_{W_{x_i}}{i_t}=\\nabla_pi_t \\nabla_qp \\nabla_{W_{x_i}}q = x_t^T\\nabla_pi_t$\n",
    "- $\\nabla_{W_{h_i}}{i_t}=\\nabla_pi_t \\nabla_vp \\nabla_{W_{h_i}}v = h_{t-1}^T\\nabla_pi_t$\n",
    "- $\\nabla_{b_i}i_t = \\nabla_pi_t \\nabla_{b_i}p = \\nabla_pi_t$\n",
    "\n",
    "## Forget gate($f$)\n",
    "$$f_t=\\sigma({W_{x_f}x_t}+{W_{h_f}h_{t-1}}+{b_f})$$\n",
    "- Partials\n",
    "\t- $f_t = \\sigma (p)$\n",
    "\t\t- $\\nabla_pf_t=(1-\\sigma(p))\\sigma(p)$\n",
    "\t- $p = q + v + b_f$\n",
    "\t\t- $\\nabla_qp=1$\n",
    "\t\t- $\\nabla_vp=1$\n",
    "\t\t- $\\nabla_{b_f}p=1$\n",
    "\t- $q = W_{x_f}x_t$\n",
    "\t\t- $\\nabla_{W_{x_f}}q = x_t^T$\n",
    "\t\t- $\\nabla_{x_t}q = W_{x_f}^T$\n",
    "\t- $v = W_{h_f}h_{t-1}$\n",
    "\t\t- $\\nabla_{W_{h_f}}q = h_{t-1}^T$\n",
    "\t\t- $\\nabla_{h_{t-1}}q = W_{h_f}^T$\n",
    "- $\\nabla_{W_{x_f}}{f_t}=\\nabla_pf_t \\nabla_qp \\nabla_{W_{x_f}}q = x_t^T\\nabla_pf_t$\n",
    "- $\\nabla_{W_{h_f}}{f_t}=\\nabla_pf_t \\nabla_vp \\nabla_{W_{h_f}}v = h_{t-1}^T\\nabla_pf_t$\n",
    "- $\\nabla_{b_f}f_t = \\nabla_pf_t \\nabla_{b_f}p = \\nabla_pf_t$\n",
    "## Output gate($o$)\n",
    "$$o_t=\\sigma({W_{x_o}x_t}+{W_{h_o}h_{t-1}}+{b_o})$$\n",
    "- Partials\n",
    "\t- $o_t = \\sigma (p)$\n",
    "\t\t- $\\nabla_po_t=(1-\\sigma(p))\\sigma(p)$\n",
    "\t- $p = q + v + b_o$\n",
    "\t\t- $\\nabla_qp=1$\n",
    "\t\t- $\\nabla_vp=1$\n",
    "\t\t- $\\nabla_{b_o}p=1$\n",
    "\t- $q = W_{x_o}x_t$\n",
    "\t\t- $\\nabla_{W_{x_o}}q = x_t^T$\n",
    "\t\t- $\\nabla_{x_t}q = W_{x_o}^T$\n",
    "\t- $v = W_{h_o}h_{t-1}$\n",
    "\t\t- $\\nabla_{W_{h_o}}q = h_{t-1}^T$\n",
    "\t\t- $\\nabla_{h_{t-1}}q = W_{h_o}^T$\n",
    "- $\\nabla_{W_{x_o}}{o_t}=\\nabla_po_t \\nabla_qp \\nabla_{W_{x_o}}q = x_t^T\\nabla_po_t$\n",
    "- $\\nabla_{W_{h_o}}{o_t}=\\nabla_po_t \\nabla_vp \\nabla_{W_{h_o}}v = h_{t-1}^T\\nabla_po_t$\n",
    "- $\\nabla_{b_o}o_t = \\nabla_po_t \\nabla_{b_o}p = \\nabla_po_t$\n",
    "## Candidate gate($g$)\n",
    "$$g_t=\\tanh({W_{x_g}x_t}+{W_{h_g}h_{t-1}}+{b_g})$$\n",
    "- Partials\n",
    "\t- $g_t = \\sigma (p)$\n",
    "\t\t- $\\nabla_pg_t=1-\\tanh^2(p)$\n",
    "\t- $p = q + v + b_g$\n",
    "\t\t- $\\nabla_qp=1$\n",
    "\t\t- $\\nabla_vp=1$\n",
    "\t\t- $\\nabla_{b_g}p=1$\n",
    "\t- $q = W_{x_g}x_t$\n",
    "\t\t- $\\nabla_{W_{x_g}}q = x_t^T$\n",
    "\t\t- $\\nabla_{x_t}q = W_{x_g}^T$\n",
    "\t- $v = W_{h_g}h_{t-1}$\n",
    "\t\t- $\\nabla_{W_{h_g}}q = h_{t-1}^T$\n",
    "\t\t- $\\nabla_{h_{t-1}}q = W_{h_g}^T$\n",
    "- $\\nabla_{W_{x_g}}{g_t}=\\nabla_pg_t \\nabla_qp \\nabla_{W_{x_g}}q = x_t^T\\nabla_pg_t$\n",
    "- $\\nabla_{W_{h_g}}{g_t}=\\nabla_pg_t \\nabla_vp \\nabla_{W_{h_g}}v = h_{t-1}^T\\nabla_pg_t$\n",
    "- $\\nabla_{b_g}g_t = \\nabla_pg_t \\nabla_{b_g}p = \\nabla_pg_t$\n",
    "## Cell State Update($c_t$)\n",
    "$$c_t = {f_t \\odot c_{t-1}}+{i_t \\odot g_t}$$\n",
    "- Partials\n",
    "\t- $c_t = q + p$\n",
    "\t\t- $\\nabla_qc_t=1$\n",
    "\t\t- $\\nabla_pc_t=1$\n",
    "\t- $q = f_t \\odot c_{t-1}$\n",
    "\t\t- $\\nabla_{f_t}q=c_{t-1}$\n",
    "\t\t- $\\nabla_{c_{t-1}}q=f_t$\n",
    "\t- $p = i_t \\odot g_t$\n",
    "\t\t- $\\nabla_{i_t}p=g_t$\n",
    "\t\t- $\\nabla_{g_t}p=i_t$\n",
    "- $\\nabla_{f_t}c_t=\\nabla_qc_t\\nabla_{f_t}q=c_{t-1}$\n",
    "- $\\nabla_{i_t}c_t= \\nabla_{p}c_t\\nabla_{i_t}p=g_t$\n",
    "- $\\nabla_{g_t}c_t= \\nabla_{p}c_t\\nabla_{g_t}p=i_t$\n",
    "\n",
    "## Hidden State Update($h_t$)\n",
    "$$h_t =o_t \\odot \\tanh(c_t)$$\n",
    "\n",
    "- Partials\n",
    "\t- $h_t = o_t \\odot q$\n",
    "\t\t- $\\nabla_{o_t}h_t=q$\n",
    "\t\t- $\\nabla_{q}h_t=o_t$\n",
    "\t- $q=\\tanh(c_t)$\n",
    "\t\t- $\\nabla_{c_t}q=1-\\tanh^2(c_t)$\n",
    "- $\\nabla_{o_t}h_t=q$\n",
    "- $\\nabla_{c_t}h_t=\\nabla_{q}h_t\\nabla_{c_t}q=o_t\\nabla_{c_t}q$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2e97f3",
   "metadata": {},
   "source": [
    "# LSTM Cell Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3136cbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atharva/Documents/sync/school/02_spring25/intro_to_deep_learning/intro-to-deep-learning/final_project/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "\n",
    "# Data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deep Learning frameworks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Dataset handling\n",
    "from datasets import load_dataset, dataset_dict\n",
    "\n",
    "# Set device for PyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb170e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/Car_data/car_data\"\n",
    "# List all CSV files in the folder\n",
    "csv_files = [\n",
    "    os.path.join(data_path, file)\n",
    "    for file in os.listdir(data_path)\n",
    "    if file.endswith(\".csv\")\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataset = dataset_dict.DatasetDict()\n",
    "\n",
    "# Load each CSV file into the dataset dict with name as the key\n",
    "for file in csv_files:\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "    # Use the filename without extension as the key\n",
    "    key = os.path.splitext(os.path.basename(file))[0]\n",
    "    dataset[key] = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f9ab33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files loaded: 9400\n",
      "All files are correct\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for file in dataset:\n",
    "    if dataset.get(file).shape != (67, 12):\n",
    "        print(f\"{file} is fucked\")\n",
    "    else:\n",
    "        i += 1\n",
    "# print(f\"Total files loaded: {len(dataset)}\")\n",
    "print(f\"Total files loaded: {i}\")\n",
    "if i == dataset.__len__():\n",
    "    print(\"All files are correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cb0fad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in dataset:\n",
    "    # print(f\"Keys in {file}: {dataset.get(file).columns.tolist()}\")\n",
    "    \n",
    "    # If this is the first file, save its keys to compare with others\n",
    "    if 'reference_keys' not in locals():\n",
    "        reference_keys = set(dataset.get(file).columns)\n",
    "    else:\n",
    "        # Check if the current file has the same keys as the reference\n",
    "        current_keys = set(dataset.get(file).columns)\n",
    "        if current_keys != reference_keys:\n",
    "            print(f\"Warning: Keys in {file} don't match the reference set!\")\n",
    "            print(f\"Different keys: {current_keys.symmetric_difference(reference_keys)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98576184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.param_init()  # Initialize parameters in constructor\n",
    "\n",
    "        # Custom LSTM Cell\n",
    "\n",
    "    def param_init(self):\n",
    "        # Initialize weights and biases\n",
    "        self.Wi = nn.Parameter(torch.Tensor(self.hidden_size, self.input_size))\n",
    "        self.Ui = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "        self.bi = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "\n",
    "        self.Wf = nn.Parameter(torch.Tensor(self.hidden_size, self.input_size))\n",
    "        self.Uf = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "        self.bf = nn.Parameter(torch.ones(self.hidden_size))\n",
    "\n",
    "        self.Wg = nn.Parameter(torch.Tensor(self.hidden_size, self.input_size))\n",
    "        self.Ug = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "        self.bg = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "\n",
    "        self.Wo = nn.Parameter(torch.Tensor(self.hidden_size, self.input_size))\n",
    "        self.Uo = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n",
    "        self.bo = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "\n",
    "    def LSTMCell(self, x_curr, h_prev, c_prev):\n",
    "        i_t = torch.sigmoid(self.Wi @ x_curr + self.Ui @ h_prev + self.bi)\n",
    "        f_t = torch.sigmoid(self.Wf @ x_curr + self.Uf @ h_prev + self.bf)\n",
    "        g_t = torch.tanh(self.Wg @ x_curr + self.Ug @ h_prev + self.bg)\n",
    "        o_t = torch.sigmoid(self.Wo @ x_curr + self.Uo @ h_prev + self.bo)\n",
    "\n",
    "        c_t = f_t * c_prev + i_t * g_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "        return h_t, c_t\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_prev = torch.zeros(self.hidden_size).to(device)\n",
    "        c_prev = torch.zeros(self.hidden_size).to(device)\n",
    "        for i in x:\n",
    "            h_prev, c_prev = self.LSTMCell(i, h_prev, c_prev)\n",
    "            y = h_prev\n",
    "        return y\n",
    "\n",
    "    def backward():\n",
    "\n",
    "        pass\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        return super().train(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e7fb14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_size=11, hidden_size=64, num_layers=1).to(device)\n",
    "\n",
    "# Keep these separate from the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
