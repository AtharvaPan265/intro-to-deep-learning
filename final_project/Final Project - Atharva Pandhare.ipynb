{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf03dff",
   "metadata": {},
   "source": [
    "# LSTM Cell Writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b83d9c",
   "metadata": {},
   "source": [
    "\n",
    "## Input Gate($i$)\n",
    "\n",
    "$$i_t = \\sigma(W_{x_i}x_t+W_{h_i}h_{t-1}+b_i))$$\n",
    "- Partials\n",
    "\t- $i_t = \\sigma (p)$\n",
    "\t\t- $\\nabla_pi_t=(1-\\sigma(p))\\sigma(p)$\n",
    "\t- $p = q + v + b_i$\n",
    "\t\t- $\\nabla_qp=1$\n",
    "\t\t- $\\nabla_vp=1$\n",
    "\t\t- $\\nabla_{b_i}p=1$\n",
    "\t- $q = W_{x_i}x_t$\n",
    "\t\t- $\\nabla_{W_{x_i}}q = x_t^T$\n",
    "\t\t- $\\nabla_{x_t}q = W_{x_i}^T$\n",
    "\t- $v = W_{h_i}h_{t-1}$\n",
    "\t\t- $\\nabla_{W_{h_i}}q = h_{t-1}^T$\n",
    "\t\t- $\\nabla_{h_{t-1}}q = W_{h_i}^T$\n",
    "- $\\nabla_{W_{x_i}}{i_t}=\\nabla_pi_t \\nabla_qp \\nabla_{W_{x_i}}q = x_t^T\\nabla_pi_t$\n",
    "- $\\nabla_{W_{h_i}}{i_t}=\\nabla_pi_t \\nabla_vp \\nabla_{W_{h_i}}v = h_{t-1}^T\\nabla_pi_t$\n",
    "- $\\nabla_{b_i}i_t = \\nabla_pi_t \\nabla_{b_i}p = \\nabla_pi_t$\n",
    "\n",
    "## Forget gate($f$)\n",
    "$$f_t=\\sigma({W_{x_f}x_t}+{W_{h_f}h_{t-1}}+{b_f})$$\n",
    "- Partials\n",
    "\t- $f_t = \\sigma (p)$\n",
    "\t\t- $\\nabla_pf_t=(1-\\sigma(p))\\sigma(p)$\n",
    "\t- $p = q + v + b_f$\n",
    "\t\t- $\\nabla_qp=1$\n",
    "\t\t- $\\nabla_vp=1$\n",
    "\t\t- $\\nabla_{b_f}p=1$\n",
    "\t- $q = W_{x_f}x_t$\n",
    "\t\t- $\\nabla_{W_{x_f}}q = x_t^T$\n",
    "\t\t- $\\nabla_{x_t}q = W_{x_f}^T$\n",
    "\t- $v = W_{h_f}h_{t-1}$\n",
    "\t\t- $\\nabla_{W_{h_f}}q = h_{t-1}^T$\n",
    "\t\t- $\\nabla_{h_{t-1}}q = W_{h_f}^T$\n",
    "- $\\nabla_{W_{x_f}}{f_t}=\\nabla_pf_t \\nabla_qp \\nabla_{W_{x_f}}q = x_t^T\\nabla_pf_t$\n",
    "- $\\nabla_{W_{h_f}}{f_t}=\\nabla_pf_t \\nabla_vp \\nabla_{W_{h_f}}v = h_{t-1}^T\\nabla_pf_t$\n",
    "- $\\nabla_{b_f}f_t = \\nabla_pf_t \\nabla_{b_f}p = \\nabla_pf_t$\n",
    "## Output gate($o$)\n",
    "$$o_t=\\sigma({W_{x_o}x_t}+{W_{h_o}h_{t-1}}+{b_o})$$\n",
    "- Partials\n",
    "\t- $o_t = \\sigma (p)$\n",
    "\t\t- $\\nabla_po_t=(1-\\sigma(p))\\sigma(p)$\n",
    "\t- $p = q + v + b_o$\n",
    "\t\t- $\\nabla_qp=1$\n",
    "\t\t- $\\nabla_vp=1$\n",
    "\t\t- $\\nabla_{b_o}p=1$\n",
    "\t- $q = W_{x_o}x_t$\n",
    "\t\t- $\\nabla_{W_{x_o}}q = x_t^T$\n",
    "\t\t- $\\nabla_{x_t}q = W_{x_o}^T$\n",
    "\t- $v = W_{h_o}h_{t-1}$\n",
    "\t\t- $\\nabla_{W_{h_o}}q = h_{t-1}^T$\n",
    "\t\t- $\\nabla_{h_{t-1}}q = W_{h_o}^T$\n",
    "- $\\nabla_{W_{x_o}}{o_t}=\\nabla_po_t \\nabla_qp \\nabla_{W_{x_o}}q = x_t^T\\nabla_po_t$\n",
    "- $\\nabla_{W_{h_o}}{o_t}=\\nabla_po_t \\nabla_vp \\nabla_{W_{h_o}}v = h_{t-1}^T\\nabla_po_t$\n",
    "- $\\nabla_{b_o}o_t = \\nabla_po_t \\nabla_{b_o}p = \\nabla_po_t$\n",
    "## Gate gate($g$)\n",
    "$$g_t=\\tanh({W_{x_g}x_t}+{W_{h_g}h_{t-1}}+{b_g})$$\n",
    "- Partials\n",
    "\t- $g_t = \\sigma (p)$\n",
    "\t\t- $\\nabla_pg_t=1-\\tanh^2(p)$\n",
    "\t- $p = q + v + b_g$\n",
    "\t\t- $\\nabla_qp=1$\n",
    "\t\t- $\\nabla_vp=1$\n",
    "\t\t- $\\nabla_{b_g}p=1$\n",
    "\t- $q = W_{x_g}x_t$\n",
    "\t\t- $\\nabla_{W_{x_g}}q = x_t^T$\n",
    "\t\t- $\\nabla_{x_t}q = W_{x_g}^T$\n",
    "\t- $v = W_{h_g}h_{t-1}$\n",
    "\t\t- $\\nabla_{W_{h_g}}q = h_{t-1}^T$\n",
    "\t\t- $\\nabla_{h_{t-1}}q = W_{h_g}^T$\n",
    "- $\\nabla_{W_{x_g}}{g_t}=\\nabla_pg_t \\nabla_qp \\nabla_{W_{x_g}}q = x_t^T\\nabla_pg_t$\n",
    "- $\\nabla_{W_{h_g}}{g_t}=\\nabla_pg_t \\nabla_vp \\nabla_{W_{h_g}}v = h_{t-1}^T\\nabla_pg_t$\n",
    "- $\\nabla_{b_g}g_t = \\nabla_pg_t \\nabla_{b_g}p = \\nabla_pg_t$\n",
    "## Cell State Update($c_t$)\n",
    "$$c_t = {f_t \\odot c_{t-1}}+{i_t \\odot g_t}$$\n",
    "- Partials\n",
    "\t- $c_t = q + p$\n",
    "\t\t- $\\nabla_qc_t=1$\n",
    "\t\t- $\\nabla_pc_t=1$\n",
    "\t- $q = f_t \\odot c_{t-1}$\n",
    "\t\t- $\\nabla_{f_t}q=c_{t-1}$\n",
    "\t\t- $\\nabla_{c_{t-1}}q=f_t$\n",
    "\t- $p = i_t \\odot g_t$\n",
    "\t\t- $\\nabla_{i_t}p=g_t$\n",
    "\t\t- $\\nabla_{g_t}p=i_t$\n",
    "- $\\nabla_{f_t}c_t=\\nabla_qc_t\\nabla_{f_t}q=c_{t-1}$\n",
    "- $\\nabla_{i_t}c_t= \\nabla_{p}c_t\\nabla_{i_t}p=g_t$\n",
    "- $\\nabla_{g_t}c_t= \\nabla_{p}c_t\\nabla_{g_t}p=i_t$\n",
    "\n",
    "## Hidden State Update($h_t$)\n",
    "$$h_t =o_t \\odot \\tanh(c_t)$$\n",
    "\n",
    "- Partials\n",
    "\t- $h_t = o_t \\odot q$\n",
    "\t\t- $\\nabla_{o_t}h_t=q$\n",
    "\t\t- $\\nabla_{q}h_t=o_t$\n",
    "\t- $q=\\tanh(c_t)$\n",
    "\t\t- $\\nabla_{c_t}q=1-\\tanh^2(c_t)$\n",
    "- $\\nabla_{o_t}h_t=q$\n",
    "- $\\nabla_{c_t}h_t=\\nabla_{q}h_t\\nabla_{c_t}q=o_t\\nabla_{c_t}q$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2e97f3",
   "metadata": {},
   "source": [
    "# LSTM Cell Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3136cbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, dataset_dict\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb170e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Local_X', 'Local_Y', 'v_Vel', 'v_Acc', 'Space_Headway', 'dis_cen',\n",
      "       'i_l', 'i_r', 'i_f', 'dis_l', 'dis_r', 'dis_f'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data/Car_data/car_data\"\n",
    "# List all CSV files in the folder\n",
    "csv_files = [os.path.join(data_path, file) for file in os.listdir(data_path) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "\n",
    "\n",
    "# Load each CSV file into a DataFrame and append to the list\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "dataset = pd.concat(dataframes, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5f9ab33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(629800, 12)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e7461",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM(nn.Module):\n",
    "    input_size = None\n",
    "    hidden_size = None\n",
    "    num_layers = None\n",
    "    dropout = None\n",
    "    # Weights and biases\n",
    "    # Input gate\n",
    "    Wi = None\n",
    "    Ui = None\n",
    "    bi = None\n",
    "    # Forget gate\n",
    "    Wf = None\n",
    "    Uf = None\n",
    "    bf = None\n",
    "    # Cell gate\n",
    "    Wg = None\n",
    "    Ug = None\n",
    "    bg = None\n",
    "    # Output gate\n",
    "    Wo = None\n",
    "    Uo = None\n",
    "    bo = None\n",
    "    # Hidden state\n",
    "    h = None\n",
    "    # Cell state\n",
    "    c = None\n",
    "\n",
    "    \n",
    "    def __init__(self, input_size=12, hidden_size = 64, num_layers=1, dropout=0.0):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        # Initialize weights and biases\n",
    "        self.init_weights(input_size, hidden_size)\n",
    "\n",
    "\n",
    "    def init_weights(self, input_size, hidden_size):  # xavier initialization\n",
    "        # Input gate parameters\n",
    "        self.Wi = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.Ui = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.bi = nn.Parameter(torch.zeros(hidden_size))\n",
    "        \n",
    "        # Forget gate parameters\n",
    "        self.Wf = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.Uf = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.bf = nn.Parameter(torch.ones(hidden_size))  # Often initialized to 1 to encourage remembering\n",
    "        \n",
    "        # Cell gate parameters\n",
    "        self.Wg = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.Ug = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.bg = nn.Parameter(torch.zeros(hidden_size))\n",
    "        \n",
    "        # Output gate parameters\n",
    "        self.Wo = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.Uo = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.bo = nn.Parameter(torch.zeros(hidden_size))\n",
    "        \n",
    "        # Apply Xavier initialization to all weight matrices\n",
    "        for weight in [self.Wi, self.Ui, self.Wf, self.Uf, self.Wg, self.Ug, self.Wo, self.Uo]:\n",
    "            nn.init.xavier_uniform_(weight)\n",
    "\n",
    "    def input_gate(self, x, h):\n",
    "        # Input gate\n",
    "        q = self.Wi @ x\n",
    "        v = self.Ui @ h\n",
    "        p = q + v + self.bi\n",
    "        i = torch.sigmoid(p)\n",
    "        return i\n",
    "    \n",
    "    def forget_gate(self, x, h):\n",
    "        # Forget gate\n",
    "        q = self.Wf @ x\n",
    "        v = self.Uf @ h\n",
    "        p = q + v + self.bf\n",
    "        f = torch.sigmoid(p)\n",
    "        return f\n",
    "\n",
    "    def output_gate(self, x, h):\n",
    "        # Output gate\n",
    "        q = self.Wo @ x\n",
    "        v = self.Uo @ h\n",
    "        p = q + v + self.bo\n",
    "        o = F.sigmoid(p)\n",
    "        return o\n",
    "\n",
    "    def cell_gate(self, x, h):\n",
    "        # Cell gate\n",
    "        q = self.Wg @ x\n",
    "        v = self.Ug @ h\n",
    "        p = q + v + self.bg\n",
    "        g = torch.tanh(p)\n",
    "        return g\n",
    "\n",
    "    def cell_state_update(self, i, f, g, c_old):\n",
    "        # Cell state update\n",
    "        c_new = f * c_old + i * g\n",
    "        return c_new\n",
    "    \n",
    "    def hidden_state_update(self, o, c_new):\n",
    "        # Hidden state update\n",
    "        h_new = o * torch.tanh(c_new)\n",
    "        return h_new\n",
    "\n",
    "    # LSTM by hand\n",
    "    def lstm_cell(self, x, h_old, c_old):\n",
    "        i = self.input_gate(x, h_old) \n",
    "        f = self.forget_gate(x, h_old)\n",
    "        g = self.cell_gate(x, h_old)\n",
    "        o = self.output_gate(x, h_old)\n",
    "\n",
    "        # New cell state\n",
    "        c_new = self.cell_state_update(i, f, g, c_old)\n",
    "        # New hidden state\n",
    "        h_new = self.hidden_state_update(o, c_new)\n",
    "        return h_new, c_new\n",
    "    \n",
    "    def forward(self, x):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98576184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
