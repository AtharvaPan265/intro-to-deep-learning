{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "In this homework we try to classify the images form the MNIST dataset, which is a collection of $70000$ hand drawn images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib import request\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "import time\n",
    "from numba import cuda\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz...\n",
      "Downloading t10k-images-idx3-ubyte.gz...\n",
      "Downloading train-labels-idx1-ubyte.gz...\n",
      "Downloading t10k-labels-idx1-ubyte.gz...\n",
      "Download complete.\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "filename = [\n",
    "[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n",
    "[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n",
    "[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n",
    "[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n",
    "]\n",
    "\n",
    "def download_mnist():\n",
    "    base_url = \"https://ossci-datasets.s3.amazonaws.com/mnist/\"\n",
    "    for name in filename:\n",
    "        print(\"Downloading \"+name[1]+\"...\")\n",
    "        request.urlretrieve(base_url+name[1], name[1])\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "def save_mnist():\n",
    "    mnist = {}\n",
    "    for name in filename[:2]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n",
    "    for name in filename[-2:]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    with open(\"mnist.pkl\", 'wb') as f:\n",
    "        pickle.dump(mnist,f)\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "def init():\n",
    "    download_mnist()\n",
    "    save_mnist()\n",
    "#    print ((load()[0]).shape)\n",
    "def load():\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbor (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load and flatten the MNIST dataset. we reshape the data to np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify using kNN\n",
    "# x_train = np.load('../x_train.npy')\n",
    "# y_train = np.load('../y_train.npy')\n",
    "# x_test = np.load('../x_test.npy')\n",
    "# y_test = np.load('../y_test.npy')\n",
    "x_train, y_train, x_test, y_test = load()\n",
    "x_train = x_train.reshape(60000, 28, 28).astype(np.float32)\n",
    "x_test = x_test.reshape(10000, 28, 28).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance Calculation**\n",
    "- L1 or Manhattan Distance is calculated by $$\\sum_{i=1}^{n} |{x_i - y_i}|$$\n",
    "- L2 or Euclidean Distance is calculated by $$\\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x1, x2, method='l2'):\n",
    "    # calculate the distance between two images\n",
    "    match method.lower():\n",
    "        case 'l1':\n",
    "            return np.sum(np.abs(x1 - x2))  # L1 norm\n",
    "        case 'l2':\n",
    "            return np.sqrt(np.sum((x1 - x2) ** 2))  # L2 norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN Classify Function**\n",
    "For this function we iterate through all the images in the testing set, then find the distance between the `testImage` and all of the `trainImage` in the `trainSet`.\n",
    "Then we have to find the K nearest for each `testImage` then we get an array `result` which is the output of the knn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNNClassify(testSet, trainSet, labels, k, method='l2'):\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for testImage in testSet:\n",
    "\n",
    "        # Calculate distances between test image and all training images\n",
    "        distances = []\n",
    "\n",
    "        for trainImage in trainSet:\n",
    "\n",
    "            #Calculate distance\n",
    "            distances.append(distance(testImage, trainImage, method))\n",
    "\n",
    "        # Find k nearest neighbors\n",
    "        distances = np.array(distances)\n",
    "        k_nearest_indices = np.argsort(distances)[:k]\n",
    "        k_nearest_labels = labels[k_nearest_indices]\n",
    "\n",
    "        # Get most common label among k neighbors\n",
    "        predicted_label = np.bincount(k_nearest_labels).argmax()\n",
    "        result.append(predicted_label)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 1000\n",
    "train = 60000\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "# for k in range(1, 13, 1):  # Test odd values of k from 1 to 13\n",
    "#     start_time = time.time()\n",
    "#     outputlabels = kNNClassify(x_test[0:test-1], x_train[0:train-1], y_train[0:train-1], k)\n",
    "#     result = y_test[0:test-1] - outputlabels\n",
    "#     accuracy = (1 - np.count_nonzero(result) / len(outputlabels))\n",
    "#     print(f\"K: {k} with accuracy: {accuracy} ran in: {(time.time() - start_time)}\\n\")\n",
    "#     if accuracy > best_accuracy:\n",
    "#         best_accuracy = accuracy\n",
    "#         best_k = k\n",
    "# print(f\"Best K: {best_k} with accuracy: {best_accuracy}\")\n",
    "\n",
    "best_k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN with L1 Distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---classification accuracy for knn on mnist: 0.9529529529529529 ---\n",
      "---execution time: 277.32752871513367 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "outputlabels = kNNClassify(x_test[0:test-1], x_train[0:train-1], y_train[0:train-1], best_k, method='l1')\n",
    "result = np.subtract(y_test[0:test-1], outputlabels)\n",
    "result = 1 - np.count_nonzero(result) / len(outputlabels)\n",
    "l1_time = time.time() - start_time\n",
    "print(\"---classification accuracy for knn on mnist: %s ---\" % result)\n",
    "print(\"---execution time: %s seconds ---\" % (l1_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN with L2 Distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---classification accuracy for knn on mnist: 0.9619619619619619 ---\n",
      "---execution time: 336.79809069633484 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "outputlabels = kNNClassify(x_test[0:test-1], x_train[0:train-1], y_train[0:train-1], best_k, method='l2')\n",
    "result = np.subtract(y_test[0:test-1], outputlabels)\n",
    "result = 1 - np.count_nonzero(result) / len(outputlabels)\n",
    "l2_time = time.time() - start_time\n",
    "print(\"---classification accuracy for knn on mnist: %s ---\" % result)\n",
    "print(\"---execution time: %s seconds ---\" % (l2_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative (Parallelized with CUDA)\n",
    "- **This allows me to run the full testing dataset, rather than just 1000 test images**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I need to schedule computation across threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit()  # this is the device funciton\n",
    "def calculate_l1_cuda(test_image, train_images, distances, n_train, image_size):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < n_train:\n",
    "        l1_dist = 0.0\n",
    "        for i in range(image_size):\n",
    "            for j in range(image_size):\n",
    "                diff = abs(test_image[i, j] - train_images[idx, i, j])\n",
    "                l1_dist += diff\n",
    "        distances[idx] = l1_dist\n",
    "\n",
    "@cuda.jit()  # this is the device funciton\n",
    "def calculate_l2_cuda(test_image, train_images, distances, n_train, image_size):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < n_train:\n",
    "        l2_dist = 0.0\n",
    "        for i in range(image_size):\n",
    "            for j in range(image_size):\n",
    "                diff = test_image[i, j] - train_images[idx, i, j]\n",
    "                l2_dist += diff * diff\n",
    "        distances[idx] = math.sqrt(l2_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding the most frequent Neighbor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent(labels):\n",
    "    values, counts = np.unique(labels, return_counts=True)\n",
    "    return values[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUDA Classify**\n",
    "Here I also have code to coput all the images to the GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this runs on the host so we need to copy training data to the device and distances to the device\n",
    "def kNNClassify_cuda(newInput, dataSet, labels, k, method=2):\n",
    "    result = []\n",
    "    n_test = len(newInput)\n",
    "    n_train = len(dataSet)\n",
    "    image_size = 28\n",
    "\n",
    "    d_train_images = cuda.to_device(dataSet)\n",
    "\n",
    "    distances = np.zeros(n_train, dtype=np.float32)\n",
    "    d_distances = cuda.to_device(distances)\n",
    "\n",
    "    threadsperblock = 256\n",
    "    blockspergrid = (n_train + (threadsperblock - 1)) // threadsperblock\n",
    "\n",
    "    for test_image in newInput:\n",
    "        d_test_image = cuda.to_device(test_image)\n",
    "        if method == 1:\n",
    "            calculate_l1_cuda[blockspergrid, threadsperblock](\n",
    "                d_test_image, d_train_images, d_distances, n_train, image_size\n",
    "            )\n",
    "        else:\n",
    "            calculate_l2_cuda[blockspergrid, threadsperblock](\n",
    "                d_test_image, d_train_images, d_distances, n_train, image_size\n",
    "            )\n",
    "\n",
    "        # Find k nearest neighbors\n",
    "        distances = np.array(d_distances.copy_to_host())\n",
    "        k_nearest_indices = np.argsort(distances)[:k]\n",
    "        k_nearest_labels = labels[k_nearest_indices]\n",
    "\n",
    "        # Get most common label among k neighbors\n",
    "        predicted_label = get_most_frequent(k_nearest_labels)\n",
    "        result.append(predicted_label)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "# for k in range(1, 13, 2):\n",
    "#     start_time = time.time()\n",
    "#     outputlabels = kNNClassify(x_test, x_train, y_train, k)\n",
    "#     result = y_test - outputlabels\n",
    "#     accuracy = (1 - np.count_nonzero(result) / len(outputlabels))\n",
    "#     print(f\"K: {k} with accuracy: {accuracy} ran in: {(time.time() - start_time)}\\n\")\n",
    "#     if accuracy > best_accuracy:\n",
    "#         best_accuracy = accuracy\n",
    "#         best_k = k\n",
    "# print(f\"Best K: {best_k} with accuracy: {best_accuracy}\")\n",
    "\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN with L1 Distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---classification accuracy for knn with L1 Distance on mnist: 0.9633 ---\n",
      "---execution time: 48.4245662689209 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "outputlabels = kNNClassify_cuda(x_test, x_train, y_train, k, method=1)\n",
    "l1_cuda_result = np.subtract(y_test, outputlabels)\n",
    "l1_cuda_result = 1 - np.count_nonzero(l1_cuda_result) / len(outputlabels)\n",
    "l1_time = time.time() - start_time\n",
    "print(\"---classification accuracy for knn with L1 Distance on mnist: %s ---\" % l1_cuda_result)\n",
    "print(\"---execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN with L2 Distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---classification accuracy for knn with L2 Distance on mnist: 0.9705 ---\n",
      "---execution time: 47.462440729141235 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "outputlabels = kNNClassify_cuda(x_test, x_train, y_train, k, method=2)\n",
    "l2_cuda_result = np.subtract(y_test, outputlabels)\n",
    "l2_cuda_result = 1 - np.count_nonzero(l2_cuda_result) / len(outputlabels)\n",
    "l2_time = time.time() - start_time\n",
    "print(\"---classification accuracy for knn with L2 Distance on mnist: %s ---\" % l2_cuda_result)\n",
    "print(\"---execution time: %s seconds ---\" % (l2_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I need to load the data as Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load() ## reload the data to convert to tensor, because the previous data is in flattened\n",
    "X_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "X_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I create the Linear Classifier as a NN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define the model as the the previously defined Linear Classifier    \n",
    "I also define the criterion as Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearClassifier()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Search** \n",
    "\n",
    "I do This by creating random tensors teh size of the weight and bias matix, then randomly iterating those tensors by adding random tensors of similar dimensions.\n",
    "\n",
    "I also use a paitience so that if the criterion doesnt improve in that paitence it stops itereating and uses the current best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(model, X_train, y_train, num_iterations):\n",
    "    W = torch.randn_like(model.linear.weight) * 0.001\n",
    "    b = torch.zeros_like(model.linear.bias)\n",
    "    bestloss = float(\"inf\")\n",
    "    patience = 20\n",
    "    no_improve = 0\n",
    "\n",
    "    # Create DataLoader for batch processing\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    loader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        step_size = 0.0001\n",
    "        # Randomly iterate weights and biases\n",
    "        Wtry = W + torch.randn_like(W) * step_size\n",
    "        btry = b + torch.randn_like(b) * step_size\n",
    "\n",
    "        # Evaluate on batches\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.linear.weight.data = Wtry\n",
    "            model.linear.bias.data = btry\n",
    "\n",
    "            for X_batch, y_batch in loader:\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        # Update if better\n",
    "        if total_loss < bestloss:\n",
    "            W = Wtry\n",
    "            b = btry\n",
    "            bestloss = total_loss\n",
    "            no_improve = 0\n",
    "            print(f\"iter {i} loss is {bestloss}\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stopping at iteration {i}\")\n",
    "            break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.linear.weight.data = W\n",
    "        model.linear.bias.data = b\n",
    "\n",
    "    return bestloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 loss is 1288.4332275390625\n",
      "iter 1 loss is 1270.5434203147888\n",
      "iter 4 loss is 1259.067527770996\n",
      "iter 9 loss is 1230.87730884552\n",
      "iter 10 loss is 1230.27095413208\n",
      "iter 11 loss is 1220.4066834449768\n",
      "iter 12 loss is 1220.3920502662659\n",
      "iter 15 loss is 1218.9924716949463\n",
      "iter 16 loss is 1202.9728779792786\n",
      "iter 18 loss is 1182.6849493980408\n",
      "iter 20 loss is 1170.8786582946777\n",
      "iter 21 loss is 1167.2236442565918\n",
      "iter 24 loss is 1161.2151064872742\n",
      "iter 25 loss is 1141.2827739715576\n",
      "iter 28 loss is 1133.6044421195984\n",
      "iter 32 loss is 1129.1107168197632\n",
      "iter 35 loss is 1121.9560375213623\n",
      "iter 36 loss is 1115.244071006775\n",
      "iter 38 loss is 1099.2066631317139\n",
      "iter 40 loss is 1093.484230041504\n",
      "iter 42 loss is 1087.5163507461548\n",
      "iter 45 loss is 1071.0399236679077\n",
      "iter 47 loss is 1063.369972229004\n",
      "iter 49 loss is 1062.0547671318054\n",
      "iter 50 loss is 1055.1246671676636\n",
      "iter 51 loss is 1053.9129929542542\n",
      "iter 55 loss is 1040.114028930664\n",
      "iter 59 loss is 1033.9718198776245\n",
      "iter 61 loss is 1022.9390466213226\n",
      "iter 63 loss is 1018.9961326122284\n",
      "iter 64 loss is 1000.5447244644165\n",
      "iter 66 loss is 992.9307098388672\n",
      "iter 67 loss is 992.7000496387482\n",
      "iter 68 loss is 988.3216812610626\n",
      "iter 69 loss is 987.5745539665222\n",
      "iter 70 loss is 965.2814466953278\n",
      "iter 72 loss is 964.4572014808655\n",
      "iter 76 loss is 961.0343999862671\n",
      "iter 78 loss is 951.8207430839539\n",
      "iter 80 loss is 935.7623882293701\n",
      "iter 85 loss is 935.7374029159546\n",
      "iter 87 loss is 927.1210474967957\n",
      "iter 89 loss is 922.1694056987762\n",
      "iter 91 loss is 918.8493301868439\n",
      "iter 95 loss is 901.6396815776825\n",
      "iter 96 loss is 899.4254925251007\n",
      "iter 97 loss is 897.951132774353\n",
      "iter 98 loss is 896.6324269771576\n",
      "iter 103 loss is 883.8245482444763\n",
      "iter 104 loss is 883.5638740062714\n",
      "iter 109 loss is 880.8111214637756\n",
      "iter 116 loss is 875.5865890979767\n",
      "iter 121 loss is 873.6108577251434\n",
      "iter 123 loss is 872.0516529083252\n",
      "iter 124 loss is 866.5483458042145\n",
      "iter 129 loss is 860.7790961265564\n",
      "iter 131 loss is 853.1220054626465\n",
      "iter 137 loss is 849.9933750629425\n",
      "iter 138 loss is 844.3208110332489\n",
      "iter 139 loss is 831.8903405666351\n",
      "iter 140 loss is 824.9401743412018\n",
      "iter 141 loss is 824.3462769985199\n",
      "iter 143 loss is 821.345267534256\n",
      "iter 144 loss is 811.8472859859467\n",
      "iter 148 loss is 810.3027901649475\n",
      "iter 150 loss is 800.9177484512329\n",
      "iter 151 loss is 794.6554229259491\n",
      "iter 154 loss is 792.5939927101135\n",
      "iter 156 loss is 783.1808862686157\n",
      "iter 157 loss is 774.675155878067\n",
      "iter 163 loss is 773.2079515457153\n",
      "iter 166 loss is 764.8127043247223\n",
      "iter 167 loss is 764.4814717769623\n",
      "iter 176 loss is 764.3180539608002\n",
      "iter 177 loss is 761.1330056190491\n",
      "iter 180 loss is 755.8228983879089\n",
      "iter 183 loss is 752.8713853359222\n",
      "iter 187 loss is 752.5123338699341\n",
      "iter 190 loss is 747.9447193145752\n",
      "iter 191 loss is 744.4795453548431\n",
      "iter 199 loss is 742.7382504940033\n",
      "iter 201 loss is 735.3550343513489\n",
      "iter 202 loss is 733.0218851566315\n",
      "iter 203 loss is 725.9162321090698\n",
      "iter 204 loss is 723.6585466861725\n",
      "iter 205 loss is 716.6221587657928\n",
      "iter 213 loss is 714.6466398239136\n",
      "iter 218 loss is 706.6507079601288\n",
      "iter 219 loss is 699.1865985393524\n",
      "iter 221 loss is 695.784202337265\n",
      "iter 222 loss is 694.782986164093\n",
      "iter 224 loss is 690.0220334529877\n",
      "iter 228 loss is 689.2609214782715\n",
      "iter 229 loss is 687.2351624965668\n",
      "iter 233 loss is 676.8126492500305\n",
      "iter 235 loss is 669.3581368923187\n",
      "iter 237 loss is 660.6123840808868\n",
      "iter 240 loss is 660.4853241443634\n",
      "iter 241 loss is 657.3488392829895\n",
      "iter 243 loss is 650.5010795593262\n",
      "iter 246 loss is 648.3858137130737\n",
      "iter 247 loss is 647.6223587989807\n",
      "iter 250 loss is 637.8301601409912\n",
      "iter 252 loss is 629.8665459156036\n",
      "iter 262 loss is 627.7561693191528\n",
      "iter 271 loss is 625.754814863205\n",
      "iter 275 loss is 625.5012240409851\n",
      "iter 276 loss is 623.3923044204712\n",
      "iter 277 loss is 622.0143835544586\n",
      "iter 282 loss is 620.2456851005554\n",
      "iter 290 loss is 618.230348110199\n",
      "iter 294 loss is 614.6198287010193\n",
      "iter 300 loss is 612.6179361343384\n",
      "iter 309 loss is 612.1239125728607\n",
      "iter 310 loss is 610.7314219474792\n",
      "iter 314 loss is 607.323171377182\n",
      "iter 315 loss is 601.9687430858612\n",
      "iter 324 loss is 599.7399728298187\n",
      "iter 325 loss is 596.2157151699066\n",
      "iter 326 loss is 593.9371485710144\n",
      "iter 342 loss is 592.7241096496582\n",
      "iter 343 loss is 585.3067893981934\n",
      "iter 344 loss is 578.8317611217499\n",
      "iter 346 loss is 575.012380361557\n",
      "iter 357 loss is 567.4642803668976\n",
      "iter 366 loss is 562.4637033939362\n",
      "iter 371 loss is 559.6287842988968\n",
      "iter 375 loss is 559.3902180194855\n",
      "iter 382 loss is 550.4123545885086\n",
      "iter 385 loss is 545.1927917003632\n",
      "iter 390 loss is 544.747209072113\n",
      "iter 395 loss is 542.0963101387024\n",
      "iter 396 loss is 540.5854127407074\n",
      "iter 402 loss is 540.3196666240692\n",
      "iter 403 loss is 536.7614328861237\n",
      "iter 404 loss is 533.4595667123795\n",
      "iter 408 loss is 529.3263318538666\n",
      "iter 409 loss is 516.3638565540314\n",
      "iter 413 loss is 512.8187826871872\n",
      "iter 414 loss is 511.74414122104645\n",
      "iter 416 loss is 511.2199356555939\n",
      "iter 417 loss is 510.2444485425949\n",
      "iter 431 loss is 507.5445672273636\n",
      "iter 434 loss is 505.7477481365204\n",
      "iter 437 loss is 502.9277161359787\n",
      "iter 439 loss is 499.3390202522278\n",
      "iter 440 loss is 493.18869853019714\n",
      "iter 443 loss is 490.9938496351242\n",
      "iter 449 loss is 486.8511756658554\n",
      "iter 450 loss is 482.0337665081024\n",
      "iter 467 loss is 478.4233908653259\n",
      "iter 469 loss is 477.48482728004456\n",
      "iter 470 loss is 473.30374777317047\n",
      "iter 471 loss is 470.38808965682983\n",
      "iter 474 loss is 469.69073009490967\n",
      "iter 478 loss is 467.1258316040039\n",
      "iter 481 loss is 467.0757415294647\n",
      "iter 482 loss is 462.38375210762024\n",
      "iter 488 loss is 461.6143077611923\n",
      "iter 495 loss is 460.83798348903656\n",
      "iter 496 loss is 457.1311032772064\n",
      "iter 501 loss is 456.4418195486069\n",
      "iter 506 loss is 449.89197397232056\n",
      "iter 511 loss is 449.5328242778778\n",
      "iter 518 loss is 448.48496866226196\n",
      "iter 522 loss is 445.78233110904694\n",
      "iter 528 loss is 445.16579580307007\n",
      "iter 529 loss is 444.7401305437088\n",
      "iter 531 loss is 443.39545500278473\n",
      "iter 536 loss is 441.76112937927246\n",
      "iter 539 loss is 434.7145800590515\n",
      "iter 541 loss is 427.741516828537\n",
      "iter 543 loss is 427.11951291561127\n",
      "iter 545 loss is 426.55107724666595\n",
      "iter 547 loss is 421.1648324728012\n",
      "iter 552 loss is 416.63090658187866\n",
      "iter 554 loss is 413.83997213840485\n",
      "iter 556 loss is 412.73089849948883\n",
      "iter 557 loss is 412.0546679496765\n",
      "iter 558 loss is 408.19606816768646\n",
      "iter 562 loss is 405.3498774766922\n",
      "iter 563 loss is 404.69526743888855\n",
      "iter 567 loss is 404.52642846107483\n",
      "iter 576 loss is 399.3685141801834\n",
      "iter 577 loss is 398.3349630832672\n",
      "iter 579 loss is 398.26232504844666\n",
      "iter 580 loss is 396.07274401187897\n",
      "iter 590 loss is 392.7993154525757\n",
      "iter 601 loss is 389.89113557338715\n",
      "iter 606 loss is 388.9317901134491\n",
      "iter 614 loss is 388.91973423957825\n",
      "iter 616 loss is 385.81501030921936\n",
      "iter 622 loss is 384.4151636362076\n",
      "iter 634 loss is 382.7047497034073\n",
      "iter 640 loss is 381.9058827161789\n",
      "iter 643 loss is 381.5646687746048\n",
      "iter 650 loss is 381.075150847435\n",
      "iter 657 loss is 381.0156410932541\n",
      "iter 661 loss is 375.99215614795685\n",
      "iter 662 loss is 375.64398419857025\n",
      "iter 671 loss is 373.20450019836426\n",
      "iter 677 loss is 372.4007457494736\n",
      "Early stopping at iteration 697\n"
     ]
    }
   ],
   "source": [
    "# Run random search\n",
    "num_iterations = 1000\n",
    "best_loss = random_search(model, X_train, y_train, num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then run Random Search over 1000 Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I can evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 55.62%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw1.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
