{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "In this homework we try to classify the images form the MNIST dataset, which is a collection of $70000$ hand drawn images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib import request\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "from download_mnist import load\n",
    "import math\n",
    "import numpy as np\n",
    "from download_mnist import load\n",
    "import operator\n",
    "import time\n",
    "from numba import cuda\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz...\n",
      "Downloading t10k-images-idx3-ubyte.gz...\n",
      "Downloading train-labels-idx1-ubyte.gz...\n",
      "Downloading t10k-labels-idx1-ubyte.gz...\n",
      "Download complete.\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "filename = [\n",
    "[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n",
    "[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n",
    "[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n",
    "[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n",
    "]\n",
    "\n",
    "def download_mnist():\n",
    "    base_url = \"https://ossci-datasets.s3.amazonaws.com/mnist/\"\n",
    "    for name in filename:\n",
    "        print(\"Downloading \"+name[1]+\"...\")\n",
    "        request.urlretrieve(base_url+name[1], name[1])\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "def save_mnist():\n",
    "    mnist = {}\n",
    "    for name in filename[:2]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n",
    "    for name in filename[-2:]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    with open(\"mnist.pkl\", 'wb') as f:\n",
    "        pickle.dump(mnist,f)\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "def init():\n",
    "    download_mnist()\n",
    "    save_mnist()\n",
    "#    print ((load()[0]).shape)\n",
    "def load():\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbor (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load and flatten the MNIST dataset. we reshape the data to np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# classify using kNN\n",
    "# x_train = np.load('../x_train.npy')\n",
    "# y_train = np.load('../y_train.npy')\n",
    "# x_test = np.load('../x_test.npy')\n",
    "# y_test = np.load('../y_test.npy')\n",
    "x_train, y_train, x_test, y_test = load()\n",
    "x_train = x_train.reshape(60000, 28, 28).astype(np.float32)\n",
    "x_test = x_test.reshape(10000, 28, 28).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance Calculation**\n",
    "- L1 or Manhattan Distance is calculated by $$\\sum_{i=1}^{n} |{x_i - y_i}|$$\n",
    "- L2 or Euclidean Distance is calculated by $$\\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x1, x2, method='l2'):\n",
    "    # calculate the distance between two images\n",
    "    match method.lower():\n",
    "        case 'l1':\n",
    "            return np.sum(np.abs(x1 - x2))  # L1 norm\n",
    "        case 'l2':\n",
    "            return np.sqrt(np.sum((x1 - x2) ** 2))  # L2 norm\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN Classify Function**\n",
    "For this function we iterate through all the images in the testing set, then find the distance between the `testImage` and all of the `trainImage` in the `trainSet`.\n",
    "Then we have to find the K nearest for each `testImage` then we get an array `result` which is the output of the knn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNNClassify(testSet, trainSet, labels, k, method='l2'):\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for testImage in testSet:\n",
    "\n",
    "        # Calculate distances between test image and all training images\n",
    "        distances = np.array([])\n",
    "\n",
    "        for trainImage in trainSet:\n",
    "\n",
    "            #Calculate distance\n",
    "            distances.append(distance(testImage, trainImage, method))\n",
    "\n",
    "        # Find k nearest neighbors\n",
    "        k_nearest_indices = np.argsort(distances)[:k]\n",
    "        k_nearest_labels = labels[k_nearest_indices]\n",
    "\n",
    "        # Get most common label among k neighbors\n",
    "        predicted_label = np.bincount(k_nearest_labels).argmax()\n",
    "        result.append(predicted_label)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 1000\n",
    "train = 60000\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "# for k in range(1, 13, 1):  # Test odd values of k from 1 to 13\n",
    "#     start_time = time.time()\n",
    "#     outputlabels = kNNClassify(x_test[0:test-1], x_train[0:train-1], y_train[0:train-1], k)\n",
    "#     result = y_test[0:test-1] - outputlabels\n",
    "#     accuracy = (1 - np.count_nonzero(result) / len(outputlabels))\n",
    "#     print(f\"K: {k} with accuracy: {accuracy} ran in: {(time.time() - start_time)}\\n\")\n",
    "#     if accuracy > best_accuracy:\n",
    "#         best_accuracy = accuracy\n",
    "#         best_k = k\n",
    "# print(f\"Best K: {best_k} with accuracy: {best_accuracy}\")\n",
    "\n",
    "best_k = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN with L1 Distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "outputlabels = kNNClassify(x_test[0:test-1], x_train[0:train-1], y_train[0:train-1], best_k, method='l1')\n",
    "result = np.subtract(y_test[0:test-1], outputlabels)\n",
    "result = 1 - np.count_nonzero(result) / len(outputlabels)\n",
    "l1_time = time.time() - start_time\n",
    "print(\"---classification accuracy for knn on mnist: %s ---\" % result)\n",
    "print(\"---execution time: %s seconds ---\" % (l1_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN with L2 Distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "outputlabels = kNNClassify(x_test[0:test-1], x_train[0:train-1], y_train[0:train-1], best_k, method='l2')\n",
    "result = np.subtract(y_test[0:test-1], outputlabels)\n",
    "result = 1 - np.count_nonzero(result) / len(outputlabels)\n",
    "l2_time = time.time() - start_time\n",
    "print(\"---classification accuracy for knn on mnist: %s ---\" % result)\n",
    "print(\"---execution time: %s seconds ---\" % (l2_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative (Parallelized with CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I need to schedule computation across threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@cuda.jit()  # this is the device funciton\n",
    "def distance_cuda(test_image, train_images, distances, n_train, image_size, method=2):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < n_train:\n",
    "        dist = 0.0\n",
    "        for i in range(image_size):\n",
    "            for j in range(image_size):\n",
    "                match method:\n",
    "                    case 1:\n",
    "                        diff = abs(test_image[i, j] - train_images[idx, i, j])\n",
    "                        dist += diff\n",
    "                    case 2:\n",
    "                        diff = test_image[i, j] - train_images[idx, i, j]\n",
    "                        dist += diff * diff\n",
    "        distances[idx] = math.sqrt(dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding the most frequent Neighbor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_most_frequent(labels):\n",
    "    values, counts = np.unique(labels, return_counts=True)\n",
    "    return values[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUDA Classify**\n",
    "Here I also have code to coput all the images to the GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this runs on the host so we need to copy training data to the device and distances to the device\n",
    "def kNNClassify(newInput, dataSet, labels, k, method=2):\n",
    "    result = []\n",
    "    n_test = len(newInput)\n",
    "    n_train = len(dataSet)\n",
    "    image_size = 28\n",
    "\n",
    "    d_train_images = cuda.to_device(dataSet)\n",
    "\n",
    "    distances = np.zeros(n_train, dtype=np.float32)\n",
    "    d_distances = cuda.to_device(distances)\n",
    "\n",
    "    threadsperblock = 256\n",
    "    blockspergrid = (n_train + (threadsperblock - 1)) // threadsperblock\n",
    "\n",
    "    for test_image in newInput:\n",
    "        d_test_image = cuda.to_device(test_image)\n",
    "        distance_cuda[blockspergrid, threadsperblock](\n",
    "            d_test_image, d_train_images, d_distances, n_train, image_size, method\n",
    "        )\n",
    "\n",
    "        # Find k nearest neighbors\n",
    "        distances = np.array(d_distances.copy_to_host())\n",
    "        k_nearest_indices = np.argsort(distances)[:k]\n",
    "        k_nearest_labels = labels[k_nearest_indices]\n",
    "\n",
    "        # Get most common label among k neighbors\n",
    "        predicted_label = get_most_frequent(k_nearest_labels)\n",
    "        result.append(predicted_label)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "# for k in range(1, 13, 2):\n",
    "#     start_time = time.time()\n",
    "#     outputlabels = kNNClassify(x_test, x_train, y_train, k)\n",
    "#     result = y_test - outputlabels\n",
    "#     accuracy = (1 - np.count_nonzero(result) / len(outputlabels))\n",
    "#     print(f\"K: {k} with accuracy: {accuracy} ran in: {(time.time() - start_time)}\\n\")\n",
    "#     if accuracy > best_accuracy:\n",
    "#         best_accuracy = accuracy\n",
    "#         best_k = k\n",
    "# print(f\"Best K: {best_k} with accuracy: {best_accuracy}\")\n",
    "\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN with L1 Distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "outputlabels = kNNClassify(x_test, x_train, y_train, k, method=1)\n",
    "l1_cuda_result = np.subtract(y_test, outputlabels)\n",
    "l1_cuda_result = 1 - np.count_nonzero(l1_cuda_result) / len(outputlabels)\n",
    "l1_time = time.time() - start_time\n",
    "print(\"---classification accuracy for knn with L1 Distance on mnist: %s ---\" % l1_cuda_result)\n",
    "print(\"---execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN with L2 Distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "outputlabels = kNNClassify(x_test, x_train, y_train, k, method=2)\n",
    "l2_cuda_result = np.subtract(y_test, outputlabels)\n",
    "l2_cuda_result = 1 - np.count_nonzero(l2_cuda_result) / len(outputlabels)\n",
    "l2_time = time.time() - start_time\n",
    "print(\"---classification accuracy for knn with L2 Distance on mnist: %s ---\" % l2_cuda_result)\n",
    "print(\"---execution time: %s seconds ---\" % (l2_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I need to load the data as Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "X_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I create the Linear Classifier as a NN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define the model as the the previously defined Linear Classifier    \n",
    "I also define the criterion as Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearClassifier()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Search** \n",
    "\n",
    "I do This by creating random tensors teh size of the weight and bias matix, then randomly iterating those tensors by adding random tensors of similar dimensions.\n",
    "\n",
    "I also use a paitience so that if the criterion doesnt improve in that paitence it stops itereating and uses the current best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(model, X_train, y_train, num_iterations):\n",
    "    W = torch.randn_like(model.linear.weight) * 0.001\n",
    "    b = torch.zeros_like(model.linear.bias)\n",
    "    bestloss = float(\"inf\")\n",
    "    patience = 20\n",
    "    no_improve = 0\n",
    "\n",
    "    # Create DataLoader for batch processing\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    loader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        step_size = 0.0001\n",
    "        # Randomly iterate weights and biases\n",
    "        Wtry = W + torch.randn_like(W) * step_size\n",
    "        btry = b + torch.randn_like(b) * step_size\n",
    "\n",
    "        # Evaluate on batches\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.linear.weight.data = Wtry\n",
    "            model.linear.bias.data = btry\n",
    "\n",
    "            for X_batch, y_batch in loader:\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        # Update if better\n",
    "        if total_loss < bestloss:\n",
    "            W = Wtry\n",
    "            b = btry\n",
    "            bestloss = total_loss\n",
    "            no_improve = 0\n",
    "            print(f\"iter {i} loss is {bestloss}\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stopping at iteration {i}\")\n",
    "            break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.linear.weight.data = W\n",
    "        model.linear.bias.data = b\n",
    "\n",
    "    return bestloss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then run Random Search over 1000 Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 loss is 1171.5546417236328\n",
      "iter 1 loss is 1152.96839427948\n",
      "iter 3 loss is 1137.916021823883\n",
      "iter 4 loss is 1127.5897722244263\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run random search\n",
    "num_iterations = 10\n",
    "best_loss = random_search(model, X_train, y_train, num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I can evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 7.49%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw1.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
