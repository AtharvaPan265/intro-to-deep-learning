{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "In this homework we try to classify the images form the MNIST dataset, which is a collection of $70000$ hand drawn images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib import request\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "import time\n",
    "from numba import cuda\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz...\n",
      "Downloading t10k-images-idx3-ubyte.gz...\n",
      "Downloading train-labels-idx1-ubyte.gz...\n",
      "Downloading t10k-labels-idx1-ubyte.gz...\n",
      "Download complete.\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "filename = [\n",
    "[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n",
    "[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n",
    "[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n",
    "[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n",
    "]\n",
    "\n",
    "def download_mnist():\n",
    "    base_url = \"https://ossci-datasets.s3.amazonaws.com/mnist/\"\n",
    "    for name in filename:\n",
    "        print(\"Downloading \"+name[1]+\"...\")\n",
    "        request.urlretrieve(base_url+name[1], name[1])\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "def save_mnist():\n",
    "    mnist = {}\n",
    "    for name in filename[:2]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n",
    "    for name in filename[-2:]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    with open(\"mnist.pkl\", 'wb') as f:\n",
    "        pickle.dump(mnist,f)\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "def init():\n",
    "    download_mnist()\n",
    "    save_mnist()\n",
    "#    print ((load()[0]).shape)\n",
    "def load():\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbor (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load and flatten the MNIST dataset. we reshape the data to np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify using kNN\n",
    "# x_train = np.load('../x_train.npy')\n",
    "# y_train = np.load('../y_train.npy')\n",
    "# x_test = np.load('../x_test.npy')\n",
    "# y_test = np.load('../y_test.npy')\n",
    "x_train, y_train, x_test, y_test = load()\n",
    "x_train = x_train.reshape(60000, 28, 28).astype(np.float32)\n",
    "x_test = x_test.reshape(10000, 28, 28).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance Calculation**\n",
    "- L1 or Manhattan Distance is calculated by $$\\sum_{i=1}^{n} |{x_i - y_i}|$$\n",
    "- L2 or Euclidean Distance is calculated by $$\\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x1, x2, method='l2'):\n",
    "    # calculate the distance between two images\n",
    "    match method.lower():\n",
    "        case 'l1':\n",
    "            return np.sum(np.abs(x1 - x2))  # L1 norm\n",
    "        case 'l2':\n",
    "            return np.sqrt(np.sum((x1 - x2) ** 2))  # L2 norm\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN Classify Function**\n",
    "For this function we iterate through all the images in the testing set, then find the distance between the `testImage` and all of the `trainImage` in the `trainSet`.\n",
    "Then we have to find the K nearest for each `testImage` then we get an array `result` which is the output of the knn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNNClassify(testSet, trainSet, labels, k, method='l2'):\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for testImage in testSet:\n",
    "\n",
    "        # Calculate distances between test image and all training images\n",
    "        distances = []\n",
    "\n",
    "        for trainImage in trainSet:\n",
    "\n",
    "            #Calculate distance\n",
    "            distances.append(distance(testImage, trainImage, method))\n",
    "\n",
    "        # Find k nearest neighbors\n",
    "        distances = np.array(distances)\n",
    "        k_nearest_indices = np.argsort(distances)[:k]\n",
    "        k_nearest_labels = labels[k_nearest_indices]\n",
    "\n",
    "        # Get most common label among k neighbors\n",
    "        predicted_label = np.bincount(k_nearest_labels).argmax()\n",
    "        result.append(predicted_label)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 1000\n",
    "train = 60000\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "# for k in range(1, 13, 1):  # Test odd values of k from 1 to 13\n",
    "#     start_time = time.time()\n",
    "#     outputlabels = kNNClassify(x_test[0:test-1], x_train[0:train-1], y_train[0:train-1], k)\n",
    "#     result = y_test[0:test-1] - outputlabels\n",
    "#     accuracy = (1 - np.count_nonzero(result) / len(outputlabels))\n",
    "#     print(f\"K: {k} with accuracy: {accuracy} ran in: {(time.time() - start_time)}\\n\")\n",
    "#     if accuracy > best_accuracy:\n",
    "#         best_accuracy = accuracy\n",
    "#         best_k = k\n",
    "# print(f\"Best K: {best_k} with accuracy: {best_accuracy}\")\n",
    "\n",
    "best_k = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN with L1 Distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m outputlabels \u001b[38;5;241m=\u001b[39m \u001b[43mkNNClassify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ml1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msubtract(y_test[\u001b[38;5;241m0\u001b[39m:test\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], outputlabels)\n\u001b[1;32m      4\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mcount_nonzero(result) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(outputlabels)\n",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m, in \u001b[0;36mkNNClassify\u001b[0;34m(testSet, trainSet, labels, k, method)\u001b[0m\n\u001b[1;32m      8\u001b[0m distances \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trainImage \u001b[38;5;129;01min\u001b[39;00m trainSet:\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#Calculate distance\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     distances\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdistance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestImage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainImage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Find k nearest neighbors\u001b[39;00m\n\u001b[1;32m     16\u001b[0m distances \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(distances)\n",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m, in \u001b[0;36mdistance\u001b[0;34m(x1, x2, method)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mmatch\u001b[39;00m method\u001b[38;5;241m.\u001b[39mlower():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 5\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# L1 norm\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39msum((x1 \u001b[38;5;241m-\u001b[39m x2) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m~/Documents/sync/school/02_spring25/intro_to_deep_learning/homeworks/HW1/hw1.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:2485\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2482\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2486\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\n\u001b[1;32m   2488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/sync/school/02_spring25/intro_to_deep_learning/homeworks/HW1/hw1.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "outputlabels = kNNClassify(x_test[0:test-1], x_train[0:train-1], y_train[0:train-1], best_k, method='l1')\n",
    "result = np.subtract(y_test[0:test-1], outputlabels)\n",
    "result = 1 - np.count_nonzero(result) / len(outputlabels)\n",
    "l1_time = time.time() - start_time\n",
    "print(\"---classification accuracy for knn on mnist: %s ---\" % result)\n",
    "print(\"---execution time: %s seconds ---\" % (l1_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN with L2 Distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "outputlabels = kNNClassify(x_test[0:test-1], x_train[0:train-1], y_train[0:train-1], best_k, method='l2')\n",
    "result = np.subtract(y_test[0:test-1], outputlabels)\n",
    "result = 1 - np.count_nonzero(result) / len(outputlabels)\n",
    "l2_time = time.time() - start_time\n",
    "print(\"---classification accuracy for knn on mnist: %s ---\" % result)\n",
    "print(\"---execution time: %s seconds ---\" % (l2_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative (Parallelized with CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I need to schedule computation across threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@cuda.jit()  # this is the device funciton\n",
    "def distance_cuda(test_image, train_images, distances, n_train, image_size, method=2):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < n_train:\n",
    "        dist = 0.0\n",
    "        for i in range(image_size):\n",
    "            for j in range(image_size):\n",
    "                match method:\n",
    "                    case 1:\n",
    "                        diff = abs(test_image[i, j] - train_images[idx, i, j])\n",
    "                        dist += diff\n",
    "                    case 2:\n",
    "                        diff = test_image[i, j] - train_images[idx, i, j]\n",
    "                        dist += diff * diff\n",
    "        distances[idx] = math.sqrt(dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding the most frequent Neighbor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_most_frequent(labels):\n",
    "    values, counts = np.unique(labels, return_counts=True)\n",
    "    return values[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUDA Classify**\n",
    "Here I also have code to coput all the images to the GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this runs on the host so we need to copy training data to the device and distances to the device\n",
    "def kNNClassify(newInput, dataSet, labels, k, method=2):\n",
    "    result = []\n",
    "    n_test = len(newInput)\n",
    "    n_train = len(dataSet)\n",
    "    image_size = 28\n",
    "\n",
    "    d_train_images = cuda.to_device(dataSet)\n",
    "\n",
    "    distances = np.zeros(n_train, dtype=np.float32)\n",
    "    d_distances = cuda.to_device(distances)\n",
    "\n",
    "    threadsperblock = 256\n",
    "    blockspergrid = (n_train + (threadsperblock - 1)) // threadsperblock\n",
    "\n",
    "    for test_image in newInput:\n",
    "        d_test_image = cuda.to_device(test_image)\n",
    "        distance_cuda[blockspergrid, threadsperblock](\n",
    "            d_test_image, d_train_images, d_distances, n_train, image_size, method\n",
    "        )\n",
    "\n",
    "        # Find k nearest neighbors\n",
    "        distances = np.array(d_distances.copy_to_host())\n",
    "        k_nearest_indices = np.argsort(distances)[:k]\n",
    "        k_nearest_labels = labels[k_nearest_indices]\n",
    "\n",
    "        # Get most common label among k neighbors\n",
    "        predicted_label = get_most_frequent(k_nearest_labels)\n",
    "        result.append(predicted_label)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "# for k in range(1, 13, 2):\n",
    "#     start_time = time.time()\n",
    "#     outputlabels = kNNClassify(x_test, x_train, y_train, k)\n",
    "#     result = y_test - outputlabels\n",
    "#     accuracy = (1 - np.count_nonzero(result) / len(outputlabels))\n",
    "#     print(f\"K: {k} with accuracy: {accuracy} ran in: {(time.time() - start_time)}\\n\")\n",
    "#     if accuracy > best_accuracy:\n",
    "#         best_accuracy = accuracy\n",
    "#         best_k = k\n",
    "# print(f\"Best K: {best_k} with accuracy: {best_accuracy}\")\n",
    "\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN with L1 Distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "outputlabels = kNNClassify(x_test, x_train, y_train, k, method=1)\n",
    "l1_cuda_result = np.subtract(y_test, outputlabels)\n",
    "l1_cuda_result = 1 - np.count_nonzero(l1_cuda_result) / len(outputlabels)\n",
    "l1_time = time.time() - start_time\n",
    "print(\"---classification accuracy for knn with L1 Distance on mnist: %s ---\" % l1_cuda_result)\n",
    "print(\"---execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN with L2 Distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "outputlabels = kNNClassify(x_test, x_train, y_train, k, method=2)\n",
    "l2_cuda_result = np.subtract(y_test, outputlabels)\n",
    "l2_cuda_result = 1 - np.count_nonzero(l2_cuda_result) / len(outputlabels)\n",
    "l2_time = time.time() - start_time\n",
    "print(\"---classification accuracy for knn with L2 Distance on mnist: %s ---\" % l2_cuda_result)\n",
    "print(\"---execution time: %s seconds ---\" % (l2_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I need to load the data as Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "X_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I create the Linear Classifier as a NN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define the model as the the previously defined Linear Classifier    \n",
    "I also define the criterion as Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearClassifier()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Search** \n",
    "\n",
    "I do This by creating random tensors teh size of the weight and bias matix, then randomly iterating those tensors by adding random tensors of similar dimensions.\n",
    "\n",
    "I also use a paitience so that if the criterion doesnt improve in that paitence it stops itereating and uses the current best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(model, X_train, y_train, num_iterations):\n",
    "    W = torch.randn_like(model.linear.weight) * 0.001\n",
    "    b = torch.zeros_like(model.linear.bias)\n",
    "    bestloss = float(\"inf\")\n",
    "    patience = 20\n",
    "    no_improve = 0\n",
    "\n",
    "    # Create DataLoader for batch processing\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    loader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        step_size = 0.0001\n",
    "        # Randomly iterate weights and biases\n",
    "        Wtry = W + torch.randn_like(W) * step_size\n",
    "        btry = b + torch.randn_like(b) * step_size\n",
    "\n",
    "        # Evaluate on batches\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.linear.weight.data = Wtry\n",
    "            model.linear.bias.data = btry\n",
    "\n",
    "            for X_batch, y_batch in loader:\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        # Update if better\n",
    "        if total_loss < bestloss:\n",
    "            W = Wtry\n",
    "            b = btry\n",
    "            bestloss = total_loss\n",
    "            no_improve = 0\n",
    "            print(f\"iter {i} loss is {bestloss}\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stopping at iteration {i}\")\n",
    "            break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.linear.weight.data = W\n",
    "        model.linear.bias.data = b\n",
    "\n",
    "    return bestloss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then run Random Search over 1000 Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run random search\n",
    "num_iterations = 10\n",
    "best_loss = random_search(model, X_train, y_train, num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I can evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw1.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
