{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "In this homework we try to classify the images form the MNIST dataset, which is a collection of $70000$ hand drawn images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib import request\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "import time\n",
    "from numba import cuda\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz...\n",
      "Downloading t10k-images-idx3-ubyte.gz...\n",
      "Downloading train-labels-idx1-ubyte.gz...\n",
      "Downloading t10k-labels-idx1-ubyte.gz...\n",
      "Download complete.\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "filename = [\n",
    "[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n",
    "[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n",
    "[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n",
    "[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n",
    "]\n",
    "\n",
    "def download_mnist():\n",
    "    base_url = \"https://ossci-datasets.s3.amazonaws.com/mnist/\"\n",
    "    for name in filename:\n",
    "        print(\"Downloading \"+name[1]+\"...\")\n",
    "        request.urlretrieve(base_url+name[1], name[1])\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "def save_mnist():\n",
    "    mnist = {}\n",
    "    for name in filename[:2]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n",
    "    for name in filename[-2:]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    with open(\"mnist.pkl\", 'wb') as f:\n",
    "        pickle.dump(mnist,f)\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "def init():\n",
    "    download_mnist()\n",
    "    save_mnist()\n",
    "#    print ((load()[0]).shape)\n",
    "def load():\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbor (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load and flatten the MNIST dataset. we reshape the data to np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify using kNN\n",
    "# x_train = np.load('../x_train.npy')\n",
    "# y_train = np.load('../y_train.npy')\n",
    "# x_test = np.load('../x_test.npy')\n",
    "# y_test = np.load('../y_test.npy')\n",
    "x_train, y_train, x_test, y_test = load()\n",
    "x_train = x_train.reshape(60000, 28, 28).astype(np.float32)\n",
    "x_test = x_test.reshape(10000, 28, 28).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance Calculation**\n",
    "- L1 or Manhattan Distance is calculated by $$\\sum_{i=1}^{n} |{x_i - y_i}|$$\n",
    "- L2 or Euclidean Distance is calculated by $$\\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x1, x2, method='l2'):\n",
    "    # calculate the distance between two images\n",
    "    match method.lower():\n",
    "        case 'l1':\n",
    "            return np.sum(np.abs(x1 - x2))  # L1 norm\n",
    "        case 'l2':\n",
    "            return np.sqrt(np.sum((x1 - x2) ** 2))  # L2 norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN Classify Function**\n",
    "For this function we iterate through all the images in the testing set, then find the distance between the `testImage` and all of the `trainImage` in the `trainSet`.\n",
    "Then we have to find the K nearest for each `testImage` then we get an array `result` which is the output of the knn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNNClassify(testSet, trainSet, labels, k, method='l2'):\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for testImage in testSet:\n",
    "\n",
    "        # Calculate distances between test image and all training images\n",
    "        distances = []\n",
    "\n",
    "        for trainImage in trainSet:\n",
    "\n",
    "            #Calculate distance\n",
    "            distances.append(distance(testImage, trainImage, method))\n",
    "\n",
    "        # Find k nearest neighbors\n",
    "        distances = np.array(distances)\n",
    "        k_nearest_indices = np.argsort(distances)[:k]\n",
    "        k_nearest_labels = labels[k_nearest_indices]\n",
    "\n",
    "        # Get most common label among k neighbors\n",
    "        predicted_label = np.bincount(k_nearest_labels).argmax()\n",
    "        result.append(predicted_label)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 1000\n",
    "train = 60000\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "# for k in range(1, 13, 1):  # Test odd values of k from 1 to 13\n",
    "#     start_time = time.time()\n",
    "#     outputlabels = kNNClassify(x_test[0:test-1], x_train[0:train-1], y_train[0:train-1], k)\n",
    "#     result = y_test[0:test-1] - outputlabels\n",
    "#     accuracy = (1 - np.count_nonzero(result) / len(outputlabels))\n",
    "#     print(f\"K: {k} with accuracy: {accuracy} ran in: {(time.time() - start_time)}\\n\")\n",
    "#     if accuracy > best_accuracy:\n",
    "#         best_accuracy = accuracy\n",
    "#         best_k = k\n",
    "# print(f\"Best K: {best_k} with accuracy: {best_accuracy}\")\n",
    "\n",
    "best_k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN with L1 Distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---classification accuracy for knn on mnist: 0.9529529529529529 ---\n",
      "---execution time: 275.7464723587036 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "outputlabels = kNNClassify(x_test[0:test-1], x_train[0:train-1], y_train[0:train-1], best_k, method='l1')\n",
    "result = np.subtract(y_test[0:test-1], outputlabels)\n",
    "result = 1 - np.count_nonzero(result) / len(outputlabels)\n",
    "l1_time = time.time() - start_time\n",
    "print(\"---classification accuracy for knn on mnist: %s ---\" % result)\n",
    "print(\"---execution time: %s seconds ---\" % (l1_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN with L2 Distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---classification accuracy for knn on mnist: 0.9619619619619619 ---\n",
      "---execution time: 337.7166998386383 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "outputlabels = kNNClassify(x_test[0:test-1], x_train[0:train-1], y_train[0:train-1], best_k, method='l2')\n",
    "result = np.subtract(y_test[0:test-1], outputlabels)\n",
    "result = 1 - np.count_nonzero(result) / len(outputlabels)\n",
    "l2_time = time.time() - start_time\n",
    "print(\"---classification accuracy for knn on mnist: %s ---\" % result)\n",
    "print(\"---execution time: %s seconds ---\" % (l2_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative (Parallelized with CUDA)\n",
    "- **This allows me to run the full testing dataset, rather than just 1000 test images**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I need to schedule computation across threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit()  # this is the device funciton\n",
    "def calculate_l1_cuda(test_image, train_images, distances, n_train, image_size):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < n_train:\n",
    "        l1_dist = 0.0\n",
    "        for i in range(image_size):\n",
    "            for j in range(image_size):\n",
    "                diff = abs(test_image[i, j] - train_images[idx, i, j])\n",
    "                l1_dist += diff\n",
    "        distances[idx] = l1_dist\n",
    "\n",
    "@cuda.jit()  # this is the device funciton\n",
    "def calculate_l2_cuda(test_image, train_images, distances, n_train, image_size):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < n_train:\n",
    "        l2_dist = 0.0\n",
    "        for i in range(image_size):\n",
    "            for j in range(image_size):\n",
    "                diff = test_image[i, j] - train_images[idx, i, j]\n",
    "                l2_dist += diff * diff\n",
    "        distances[idx] = math.sqrt(l2_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding the most frequent Neighbor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent(labels):\n",
    "    values, counts = np.unique(labels, return_counts=True)\n",
    "    return values[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUDA Classify**\n",
    "Here I also have code to coput all the images to the GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this runs on the host so we need to copy training data to the device and distances to the device\n",
    "def kNNClassify_cuda(newInput, dataSet, labels, k, method=2):\n",
    "    result = []\n",
    "    n_test = len(newInput)\n",
    "    n_train = len(dataSet)\n",
    "    image_size = 28\n",
    "\n",
    "    d_train_images = cuda.to_device(dataSet)\n",
    "\n",
    "    distances = np.zeros(n_train, dtype=np.float32)\n",
    "    d_distances = cuda.to_device(distances)\n",
    "\n",
    "    threadsperblock = 256\n",
    "    blockspergrid = (n_train + (threadsperblock - 1)) // threadsperblock\n",
    "\n",
    "    for test_image in newInput:\n",
    "        d_test_image = cuda.to_device(test_image)\n",
    "        if method == 1:\n",
    "            calculate_l1_cuda[blockspergrid, threadsperblock](\n",
    "                d_test_image, d_train_images, d_distances, n_train, image_size\n",
    "            )\n",
    "        else:\n",
    "            calculate_l2_cuda[blockspergrid, threadsperblock](\n",
    "                d_test_image, d_train_images, d_distances, n_train, image_size\n",
    "            )\n",
    "\n",
    "        # Find k nearest neighbors\n",
    "        distances = np.array(d_distances.copy_to_host())\n",
    "        k_nearest_indices = np.argsort(distances)[:k]\n",
    "        k_nearest_labels = labels[k_nearest_indices]\n",
    "\n",
    "        # Get most common label among k neighbors\n",
    "        predicted_label = get_most_frequent(k_nearest_labels)\n",
    "        result.append(predicted_label)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "# for k in range(1, 13, 2):\n",
    "#     start_time = time.time()\n",
    "#     outputlabels = kNNClassify(x_test, x_train, y_train, k)\n",
    "#     result = y_test - outputlabels\n",
    "#     accuracy = (1 - np.count_nonzero(result) / len(outputlabels))\n",
    "#     print(f\"K: {k} with accuracy: {accuracy} ran in: {(time.time() - start_time)}\\n\")\n",
    "#     if accuracy > best_accuracy:\n",
    "#         best_accuracy = accuracy\n",
    "#         best_k = k\n",
    "# print(f\"Best K: {best_k} with accuracy: {best_accuracy}\")\n",
    "\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN with L1 Distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---classification accuracy for knn with L1 Distance on mnist: 0.9633 ---\n",
      "---execution time: 46.68482303619385 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "outputlabels = kNNClassify_cuda(x_test, x_train, y_train, k, method=1)\n",
    "l1_cuda_result = np.subtract(y_test, outputlabels)\n",
    "l1_cuda_result = 1 - np.count_nonzero(l1_cuda_result) / len(outputlabels)\n",
    "l1_time = time.time() - start_time\n",
    "print(\"---classification accuracy for knn with L1 Distance on mnist: %s ---\" % l1_cuda_result)\n",
    "print(\"---execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN with L2 Distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---classification accuracy for knn with L2 Distance on mnist: 0.9705 ---\n",
      "---execution time: 47.02558994293213 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "outputlabels = kNNClassify_cuda(x_test, x_train, y_train, k, method=2)\n",
    "l2_cuda_result = np.subtract(y_test, outputlabels)\n",
    "l2_cuda_result = 1 - np.count_nonzero(l2_cuda_result) / len(outputlabels)\n",
    "l2_time = time.time() - start_time\n",
    "print(\"---classification accuracy for knn with L2 Distance on mnist: %s ---\" % l2_cuda_result)\n",
    "print(\"---execution time: %s seconds ---\" % (l2_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I need to load the data as Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load() ## reload the data to convert to tensor, because the previous data is in flattened\n",
    "X_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "X_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I create the Linear Classifier as a NN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define the model as the the previously defined Linear Classifier    \n",
    "I also define the criterion as Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearClassifier()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Search** \n",
    "\n",
    "I do This by creating random tensors teh size of the weight and bias matix, then randomly iterating those tensors by adding random tensors of similar dimensions.\n",
    "\n",
    "I also use a paitience so that if the criterion doesnt improve in that paitence it stops itereating and uses the current best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(model, X_train, y_train, num_iterations):\n",
    "    W = torch.randn_like(model.linear.weight) * 0.001\n",
    "    b = torch.zeros_like(model.linear.bias)\n",
    "    bestloss = float(\"inf\")\n",
    "    patience = 20\n",
    "    no_improve = 0\n",
    "\n",
    "    # Create DataLoader for batch processing\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    loader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        step_size = 0.0001\n",
    "        # Randomly iterate weights and biases\n",
    "        Wtry = W + torch.randn_like(W) * step_size\n",
    "        btry = b + torch.randn_like(b) * step_size\n",
    "\n",
    "        # Evaluate on batches\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.linear.weight.data = Wtry\n",
    "            model.linear.bias.data = btry\n",
    "\n",
    "            for X_batch, y_batch in loader:\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        # Update if better\n",
    "        if total_loss < bestloss:\n",
    "            W = Wtry\n",
    "            b = btry\n",
    "            bestloss = total_loss\n",
    "            no_improve = 0\n",
    "            print(f\"iter {i} loss is {bestloss}\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stopping at iteration {i}\")\n",
    "            break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.linear.weight.data = W\n",
    "        model.linear.bias.data = b\n",
    "\n",
    "    return bestloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 loss is 972.9965524673462\n",
      "iter 3 loss is 971.1871347427368\n",
      "iter 5 loss is 970.7853734493256\n",
      "iter 7 loss is 953.9483580589294\n",
      "iter 8 loss is 948.4549331665039\n",
      "iter 10 loss is 938.0834476947784\n",
      "iter 11 loss is 921.4819538593292\n",
      "iter 12 loss is 912.2363419532776\n",
      "iter 13 loss is 891.3741278648376\n",
      "iter 25 loss is 887.4673926830292\n",
      "iter 27 loss is 885.2469449043274\n",
      "iter 28 loss is 881.9186415672302\n",
      "iter 31 loss is 873.1291332244873\n",
      "iter 32 loss is 865.4674007892609\n",
      "iter 37 loss is 860.9611802101135\n",
      "iter 41 loss is 858.6912062168121\n",
      "iter 42 loss is 852.3809564113617\n",
      "iter 49 loss is 852.3537600040436\n",
      "iter 51 loss is 851.4858276844025\n",
      "iter 52 loss is 843.6579508781433\n",
      "iter 62 loss is 843.0981078147888\n",
      "iter 66 loss is 838.7063837051392\n",
      "iter 70 loss is 833.1891491413116\n",
      "iter 73 loss is 817.394223690033\n",
      "iter 91 loss is 807.0933244228363\n",
      "iter 94 loss is 804.0488550662994\n",
      "iter 96 loss is 799.809755563736\n",
      "iter 97 loss is 799.7234072685242\n",
      "iter 100 loss is 792.5128192901611\n",
      "iter 104 loss is 789.1803686618805\n",
      "iter 105 loss is 789.1078298091888\n",
      "iter 106 loss is 786.9032552242279\n",
      "iter 109 loss is 776.5120255947113\n",
      "iter 112 loss is 769.8213701248169\n",
      "iter 116 loss is 768.878169298172\n",
      "iter 119 loss is 768.7650716304779\n",
      "iter 123 loss is 765.5589973926544\n",
      "iter 127 loss is 756.1874144077301\n",
      "iter 129 loss is 751.1587266921997\n",
      "iter 130 loss is 742.4206235408783\n",
      "iter 135 loss is 731.8277344703674\n",
      "iter 139 loss is 729.2146506309509\n",
      "iter 141 loss is 712.3126292228699\n",
      "iter 142 loss is 711.9655363559723\n",
      "iter 143 loss is 711.7835221290588\n",
      "iter 144 loss is 709.7122802734375\n",
      "iter 158 loss is 706.3389110565186\n",
      "iter 159 loss is 703.022952079773\n",
      "iter 165 loss is 696.4874119758606\n",
      "iter 166 loss is 690.8733220100403\n",
      "iter 168 loss is 689.5666244029999\n",
      "iter 172 loss is 679.9857468605042\n",
      "iter 177 loss is 677.1229717731476\n",
      "iter 178 loss is 675.4020781517029\n",
      "iter 181 loss is 668.9591097831726\n",
      "iter 185 loss is 656.3398959636688\n",
      "iter 189 loss is 652.8328516483307\n",
      "iter 191 loss is 651.8487560749054\n",
      "iter 194 loss is 645.1855752468109\n",
      "iter 195 loss is 644.1110932826996\n",
      "iter 196 loss is 640.193487405777\n",
      "iter 198 loss is 638.8000872135162\n",
      "iter 199 loss is 631.491781949997\n",
      "iter 206 loss is 629.029390335083\n",
      "iter 208 loss is 626.6619520187378\n",
      "iter 210 loss is 624.8346736431122\n",
      "iter 217 loss is 623.2165613174438\n",
      "iter 219 loss is 618.9623892307281\n",
      "iter 221 loss is 614.773638010025\n",
      "iter 224 loss is 613.8875856399536\n",
      "iter 225 loss is 610.6387121677399\n",
      "iter 233 loss is 610.4425909519196\n",
      "iter 234 loss is 609.6140298843384\n",
      "iter 238 loss is 605.3239204883575\n",
      "iter 241 loss is 601.6308274269104\n",
      "iter 244 loss is 593.9261500835419\n",
      "iter 253 loss is 588.1044182777405\n",
      "iter 259 loss is 584.8424837589264\n",
      "iter 263 loss is 578.1553497314453\n",
      "iter 265 loss is 574.5319938659668\n",
      "iter 269 loss is 572.1391398906708\n",
      "iter 270 loss is 570.4906339645386\n",
      "iter 271 loss is 569.0766112804413\n",
      "iter 280 loss is 568.3956248760223\n",
      "iter 282 loss is 566.5225369930267\n",
      "iter 288 loss is 562.7425298690796\n",
      "iter 296 loss is 562.3144805431366\n",
      "iter 301 loss is 561.9298346042633\n",
      "iter 303 loss is 556.2889078855515\n",
      "iter 305 loss is 553.7046575546265\n",
      "iter 311 loss is 553.3110675811768\n",
      "iter 316 loss is 548.9126040935516\n",
      "iter 320 loss is 548.4304649829865\n",
      "iter 321 loss is 547.7561362981796\n",
      "iter 326 loss is 545.4244694709778\n",
      "iter 333 loss is 542.9867570400238\n",
      "iter 335 loss is 540.2695699930191\n",
      "iter 343 loss is 540.1204762458801\n",
      "iter 344 loss is 538.5413639545441\n",
      "iter 346 loss is 533.8516862392426\n",
      "iter 362 loss is 530.4737641811371\n",
      "iter 374 loss is 525.5853114128113\n",
      "iter 376 loss is 524.6620662212372\n",
      "iter 381 loss is 523.2887961864471\n",
      "iter 383 loss is 520.5795179605484\n",
      "iter 385 loss is 518.2775642871857\n",
      "iter 390 loss is 515.551144361496\n",
      "iter 391 loss is 510.74187767505646\n",
      "iter 395 loss is 504.51985573768616\n",
      "iter 397 loss is 501.6357589960098\n",
      "iter 399 loss is 497.73357629776\n",
      "iter 403 loss is 497.65443181991577\n",
      "iter 406 loss is 497.6275441646576\n",
      "iter 409 loss is 495.30923891067505\n",
      "iter 418 loss is 495.12645173072815\n",
      "iter 435 loss is 493.0992373228073\n",
      "iter 439 loss is 490.2086853981018\n",
      "iter 445 loss is 484.4991374015808\n",
      "iter 452 loss is 481.06915187835693\n",
      "iter 456 loss is 479.9139897823334\n",
      "iter 466 loss is 477.37454903125763\n",
      "iter 478 loss is 477.1031663417816\n",
      "iter 480 loss is 473.21530425548553\n",
      "iter 485 loss is 472.72734010219574\n",
      "iter 487 loss is 472.34826827049255\n",
      "iter 490 loss is 471.5476804971695\n",
      "iter 493 loss is 469.43462109565735\n",
      "iter 495 loss is 468.73708939552307\n",
      "iter 498 loss is 464.43986761569977\n",
      "iter 500 loss is 456.5877398252487\n",
      "iter 501 loss is 452.9402903318405\n",
      "iter 507 loss is 451.63563573360443\n",
      "iter 508 loss is 449.42149209976196\n",
      "iter 513 loss is 449.30492627620697\n",
      "iter 514 loss is 448.1386889219284\n",
      "iter 517 loss is 447.5015195608139\n",
      "iter 519 loss is 444.71901881694794\n",
      "iter 527 loss is 441.0835210084915\n",
      "iter 537 loss is 440.78677928447723\n",
      "iter 538 loss is 439.96349024772644\n",
      "iter 539 loss is 439.44799995422363\n",
      "iter 544 loss is 439.18560540676117\n",
      "iter 559 loss is 438.9425200223923\n",
      "iter 563 loss is 437.8042559623718\n",
      "iter 571 loss is 436.71906316280365\n",
      "iter 575 loss is 436.51673781871796\n",
      "iter 576 loss is 431.48624312877655\n",
      "iter 580 loss is 431.1464195251465\n",
      "iter 584 loss is 431.04766070842743\n",
      "iter 586 loss is 427.5263440608978\n",
      "iter 587 loss is 427.23269963264465\n",
      "iter 591 loss is 424.09798848629\n",
      "iter 606 loss is 422.65196192264557\n",
      "iter 608 loss is 418.65102314949036\n",
      "iter 611 loss is 416.73304307460785\n",
      "iter 616 loss is 412.46741580963135\n",
      "iter 618 loss is 410.88068103790283\n",
      "iter 621 loss is 406.4417848587036\n",
      "iter 623 loss is 406.1593087911606\n",
      "iter 625 loss is 404.69087970256805\n",
      "iter 626 loss is 404.5505175590515\n",
      "iter 640 loss is 401.39259934425354\n",
      "iter 644 loss is 400.3712797164917\n",
      "iter 645 loss is 400.05269384384155\n",
      "iter 649 loss is 398.5798667669296\n",
      "iter 650 loss is 397.8381208181381\n",
      "iter 651 loss is 397.21473145484924\n",
      "iter 670 loss is 394.4460771083832\n",
      "iter 687 loss is 393.0947366952896\n",
      "iter 694 loss is 392.2623220682144\n",
      "iter 701 loss is 390.69711470603943\n",
      "iter 702 loss is 387.9499179124832\n",
      "iter 704 loss is 385.8403527736664\n",
      "iter 708 loss is 383.4819859266281\n",
      "Early stopping at iteration 728\n"
     ]
    }
   ],
   "source": [
    "# Run random search\n",
    "num_iterations = 1000\n",
    "best_loss = random_search(model, X_train, y_train, num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then run Random Search over 1000 Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I can evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 52.55%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw1.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
