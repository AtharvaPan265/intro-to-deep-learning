{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "Use back-propagation to calculate the gradients of $$f(W,x)=||\\sigma(Wx)||^2$$\n",
    "with respect to x and W. Here, $∥\\cdot∥^2$ is the calculation of L2 loss, $W$ is a $3×3$ matrix, and $x$ is a $3 × 1$ vector, and $\\sigma(\\cdot)$ is the ReLU function that performs element-wise operation.\n",
    "\n",
    "$$\n",
    "W = \n",
    "\\begin{bmatrix}\n",
    "W_{1,1} & W_{1,2} & W_{1,3}\\\\\n",
    "W_{2,1} & W_{2,2} & W_{2,3}\\\\\n",
    "W_{3,1} & W_{3,2} & W_{3,3}\n",
    "\\end{bmatrix},\\ \\ x = \\begin{bmatrix} x_{1}\\\\ x_{2}\\\\ x_{3} \\end{bmatrix}\n",
    "$$\n",
    "Let's say:\n",
    "$$\n",
    "z = \\begin{bmatrix}\n",
    "W_{1,1}x_1 + W_{1,2}x_2 + W_{1,3}x_3\\\\\n",
    "W_{2,1}x_1 + W_{2,2}x_2 + W_{2,3}x_3\\\\\n",
    "W_{3,1}x_1 + W_{3,2}x_2 + W_{3,3}x_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "z_{1}\\\\\n",
    "z_{2}\\\\\n",
    "z_{3}\n",
    "\\end{bmatrix}\n",
    "\n",
    "$$\n",
    "so to do $a=\\sigma(z)$\n",
    "$$\n",
    "a = \\begin{bmatrix}\n",
    "max(0,z_1)\\\\\n",
    "max(0,z_2)\\\\\n",
    "max(0,z_3)\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "a_{1}\\\\\n",
    "a_{2}\\\\\n",
    "a_{3}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$a =\\begin{cases} z_i & z_i > 0 \\\\ 0 & z_i \\leq 0 \\end{cases} $$\n",
    "Now we are left with $$f(W,x)=||a||^2$$\n",
    "then Gradient with respect to $\\mathbf{a}$ \n",
    "$$\\dfrac{\\partial f}{\\partial a_i} = \\dfrac{\\partial }{\\partial a}(a_1^2 + a_2^2 + a_3^2) = 2a_i = \n",
    "\\begin{bmatrix}\n",
    "2a_{1}\\\\\n",
    "2a_{2}\\\\\n",
    "2a_{3}\n",
    "\\end{bmatrix}$$\n",
    "so we get the gradient of $f$ with respect to $a$ $$\\nabla_af=2a$$\n",
    "then we want to find  $\\nabla_zf$\n",
    "$$\\dfrac{\\partial{f}}{\\partial{z}}=\\dfrac{\\partial{f}}{\\partial{a}}\\dfrac{\\partial{a}}{\\partial{z}}$$\n",
    "and we know derivative of the ReLU is:\n",
    "$$\\dfrac{\\partial{a}}{\\partial{z}} = \\begin{cases}1 & z_i > 0 \\\\ 0  & z_i \\leq 0 \\end{cases} = \\begin{bmatrix}\n",
    "I_{(z_1>0)} &0&0\\\\\n",
    "0&I_{(z_2>0)}&0\\\\\n",
    "0&0&I_{(z_3>0)}\n",
    "\\end{bmatrix}$$\n",
    "so we can get \n",
    "$$\\nabla_zf=\\dfrac{\\partial{f}}{\\partial{z}}= \\begin{bmatrix}2a_1 \\cdot I(z_1 > 0) \\\\ 2a_2 \\cdot I(z_2 > 0) \\\\ 2a_3 \\cdot I(z_3 > 0) \\end{bmatrix} = \n",
    "\\begin{bmatrix}2a_1\\ if\\ z_1 > 0,\\ else\\  0 \\\\2a_2\\ if\\ z_2 > 0,\\ else\\  0 \\\\2a_3\\ if\\ z_3 > 0,\\ else\\  0  \\end{bmatrix} = 2a\\cdot I_{z>0}$$ Now to find $\\nabla_x f$ \n",
    "$$\\dfrac{\\partial{f}}{\\partial{x}}=\\dfrac{\\partial{f}}{\\partial{z}}\\dfrac{\\partial{z}}{\\partial{x}}$$\n",
    "we know can find the $\\dfrac{\\partial{z}}{\\partial{x}}$ which is :\n",
    "$$\\dfrac{\\partial{z_k}}{\\partial{x_i}}=W_{k,i}$$\n",
    "\n",
    "so\n",
    "$$\\nabla_xf = \\dfrac{\\partial{f}}{\\partial{x}}=\\sum_j\\dfrac{\\partial{f}}{\\partial{z_j}}\\dfrac{\\partial{z_j}}{\\partial{x_i}}=\\sum_j2z_i\\cdot I_{(z_i>0)} W_{j,i}=W^T\\cdot \\nabla_zf$$\n",
    "on the other hand $\\nabla_Wf$ is much easier to find\n",
    "$$\\nabla_Wf = \\nabla_zf \\cdot x^T$$  \n",
    "\n",
    "---\n",
    "\n",
    "# Problem 2\n",
    "In this problem, you need to use Gradient Descent (GD) to train the linear classifier in the HW1, i.e., find the parameters W , and then use it to recognize handwritten digits. Adopt still ”Cross Entropy” as the loss function.\n",
    "\n",
    "Requirements: \n",
    "1) manually derive the gradients of linear classifier when using cross-entropy as the loss function, and write codes to implement it in recognizing handwritten digits\n",
    "2) the test accuracy should be at least 85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib import request\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz...\n",
      "Downloading t10k-images-idx3-ubyte.gz...\n",
      "Downloading train-labels-idx1-ubyte.gz...\n",
      "Downloading t10k-labels-idx1-ubyte.gz...\n",
      "Download complete.\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "filename = [\n",
    "[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n",
    "[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n",
    "[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n",
    "[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n",
    "]\n",
    "\n",
    "def download_mnist():\n",
    "    base_url = \"https://ossci-datasets.s3.amazonaws.com/mnist/\"\n",
    "    for name in filename:\n",
    "        print(\"Downloading \"+name[1]+\"...\")\n",
    "        request.urlretrieve(base_url+name[1], name[1])\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "def save_mnist():\n",
    "    mnist = {}\n",
    "    for name in filename[:2]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n",
    "    for name in filename[-2:]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    with open(\"mnist.pkl\", 'wb') as f:\n",
    "        pickle.dump(mnist,f)\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "def init():\n",
    "    download_mnist()\n",
    "    save_mnist()\n",
    "#    print ((load()[0]).shape)\n",
    "def load():\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I need to load the data as Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load() ## reload the data to convert to tensor, because the previous data is in flattened\n",
    "X_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "X_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I create the Linear Classifier as a NN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define the model as the the previously defined Linear Classifier    \n",
    "I also define the criterion as Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearClassifier()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Gradient(SG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
