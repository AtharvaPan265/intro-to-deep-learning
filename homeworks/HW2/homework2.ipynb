{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "Use back-propagation to calculate the gradients of $$f(W,x)=||\\sigma(Wx)||^2$$\n",
    "with respect to x and W. Here, $∥\\cdot∥^2$ is the calculation of L2 loss, $W$ is a $3×3$ matrix, and $x$ is a $3 × 1$ vector, and $\\sigma(\\cdot)$ is the ReLU function that performs element-wise operation.\n",
    "\n",
    "We can first write out $W$ and $x$\n",
    "\n",
    "$$\n",
    "W = \n",
    "\\begin{bmatrix}\n",
    "W_{1,1} & W_{1,2} & W_{1,3}\\\\\n",
    "W_{2,1} & W_{2,2} & W_{2,3}\\\\\n",
    "W_{3,1} & W_{3,2} & W_{3,3}\n",
    "\\end{bmatrix},\\ \\ x = \\begin{bmatrix} x_{1}\\\\ x_{2}\\\\ x_{3} \\end{bmatrix}\n",
    "$$\n",
    "Let's say:\n",
    "$$\n",
    "z = \\begin{bmatrix}\n",
    "W_{1,1}x_1 + W_{1,2}x_2 + W_{1,3}x_3\\\\\n",
    "W_{2,1}x_1 + W_{2,2}x_2 + W_{2,3}x_3\\\\\n",
    "W_{3,1}x_1 + W_{3,2}x_2 + W_{3,3}x_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "z_{1}\\\\\n",
    "z_{2}\\\\\n",
    "z_{3}\n",
    "\\end{bmatrix}\n",
    "\n",
    "$$\n",
    "so to do $a=\\sigma(z)$\n",
    "$$\n",
    "a = \n",
    "\\begin{bmatrix}\n",
    "max(0,z_1)\\\\\n",
    "max(0,z_2)\\\\\n",
    "max(0,z_3)\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "a_{1}\\\\\n",
    "a_{2}\\\\\n",
    "a_{3}\n",
    "\\end{bmatrix}  =\n",
    "\\begin{cases} z_i & z_i > 0 \\\\ 0 & z_i \\leq 0 \\end{cases} $$\n",
    "\n",
    "\n",
    "Now we are left with $$f(W,x)=||a||^2$$\n",
    "then Gradient with respect to $\\mathbf{a}$ \n",
    "$$\\dfrac{\\partial f}{\\partial a} = \\dfrac{\\partial }{\\partial a}(a_1^2 + a_2^2 + a_3^2) = 2a = \n",
    "\\begin{bmatrix}\n",
    "2a_{1}\\\\\n",
    "2a_{2}\\\\\n",
    "2a_{3}\n",
    "\\end{bmatrix}$$\n",
    "so we get the gradient of $f$ with respect to $a$ $$\\nabla_af=2a$$\n",
    "then we want to find  $\\nabla_zf$\n",
    "$$\\dfrac{\\partial{f}}{\\partial{z}}=\\dfrac{\\partial{f}}{\\partial{a}}\\dfrac{\\partial{a}}{\\partial{z}}$$\n",
    "and we know derivative of the ReLU is:\n",
    "$$\\dfrac{\\partial{a}}{\\partial{z}} = \\begin{cases}1 & z_i > 0 \\\\ 0  & z_i \\leq 0 \\end{cases} = \\begin{bmatrix}\n",
    "I_{(z_1>0)} &0&0\\\\\n",
    "0&I_{(z_2>0)}&0\\\\\n",
    "0&0&I_{(z_3>0)}\n",
    "\\end{bmatrix}$$\n",
    "so we can get \n",
    "$$\\nabla_zf=\\dfrac{\\partial{f}}{\\partial{z}}= \\begin{bmatrix}2a_1 \\cdot I(z_1 > 0) \\\\ 2a_2 \\cdot I(z_2 > 0) \\\\ 2a_3 \\cdot I(z_3 > 0) \\end{bmatrix} = \n",
    "\\begin{bmatrix}2a_1\\ if\\ z_1 > 0,\\ else\\  0 \\\\2a_2\\ if\\ z_2 > 0,\\ else\\  0 \\\\2a_3\\ if\\ z_3 > 0,\\ else\\  0  \\end{bmatrix} = 2a\\cdot I_{z>0}$$ Now to find $\\nabla_x f$ \n",
    "$$\\dfrac{\\partial{f}}{\\partial{x}}=\\dfrac{\\partial{f}}{\\partial{z}}\\dfrac{\\partial{z}}{\\partial{x}}$$\n",
    "we know can find the $\\dfrac{\\partial{z}}{\\partial{x}}$ which is :\n",
    "$$\\dfrac{\\partial{z_k}}{\\partial{x_i}}=W_{k,i}$$\n",
    "\n",
    "so\n",
    "$$\\nabla_xf = \n",
    "\\dfrac{\\partial{f}}{\\partial{x}}=\n",
    "\\sum_j\\dfrac{\\partial{f}}{\\partial{z_j}}\\dfrac{\\partial{z_j}}{\\partial{x_i}}=\n",
    "\\sum_j2z_i\\cdot I_{(z_i>0)} W_{j,i} =\n",
    "\\nabla_zf \\cdot W^T$$\n",
    "on the other hand $\\nabla_Wf$ is much easier to find\n",
    "$$\\nabla_Wf = \\nabla_zf \\cdot x^T$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Problem 2\n",
    "In this problem, you need to use Gradient Descent (GD) to train the linear classifier in the HW1, i.e., find the parameters W , and then use it to recognize handwritten digits. Adopt still ”Cross Entropy” as the loss function.\n",
    "\n",
    "Requirements: \n",
    "1) manually derive the gradients of linear classifier when using cross-entropy as the loss function, and write codes to implement it in recognizing handwritten digits\n",
    "2) the test accuracy should be at least 85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib import request\n",
    "import gzip\n",
    "import pickle\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz...\n",
      "Downloading t10k-images-idx3-ubyte.gz...\n",
      "Downloading train-labels-idx1-ubyte.gz...\n",
      "Downloading t10k-labels-idx1-ubyte.gz...\n",
      "Download complete.\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "filename = [\n",
    "[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n",
    "[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n",
    "[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n",
    "[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n",
    "]\n",
    "\n",
    "def download_mnist():\n",
    "    base_url = \"https://ossci-datasets.s3.amazonaws.com/mnist/\"\n",
    "    for name in filename:\n",
    "        print(\"Downloading \"+name[1]+\"...\")\n",
    "        request.urlretrieve(base_url+name[1], name[1])\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "def save_mnist():\n",
    "    mnist = {}\n",
    "    for name in filename[:2]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n",
    "    for name in filename[-2:]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    with open(\"mnist.pkl\", 'wb') as f:\n",
    "        pickle.dump(mnist,f)\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "def init():\n",
    "    download_mnist()\n",
    "    save_mnist()\n",
    "#    print ((load()[0]).shape)\n",
    "def load():\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n",
    "\n",
    "init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load()\n",
    "x_train = x_train.reshape(60000, 28 * 28)/255\n",
    "y_train = y_train.reshape(60000)\n",
    "x_test = x_test.reshape(10000, 28 * 28)/255\n",
    "y_test = y_test.reshape(10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Classifier Class Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier:\n",
    "    def __init__(self, inputDim=784, outputDim=10, lr=0.01):\n",
    "        # Initialize weights and bias\n",
    "        self.W = np.random.randn(inputDim, outputDim) * 0.01  # small random numbers\n",
    "        self.b = np.zeros(outputDim)\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear forward pass\n",
    "        scores = x @ self.W + self.b\n",
    "        return scores\n",
    "\n",
    "    def softmax(self, s):\n",
    "        exps = np.exp(s - np.max(s, axis=1, keepdims=True))\n",
    "        probs = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        return probs\n",
    "\n",
    "    def predict(self, data):\n",
    "        scores = self.forward(data)\n",
    "        probs = self.softmax(scores)\n",
    "        predictions = np.argmax(probs, axis=1)\n",
    "        return predictions, probs\n",
    "\n",
    "    def crossEntropyLoss(self, probs, labels):\n",
    "        N=len(labels)\n",
    "        loss = -np.mean(np.log(probs[np.arange(N), labels] + 1e-8))\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def backward(self, x, y, probs):\n",
    "        N = len(y)\n",
    "        # Compute gradient for softmax with cross-entropy loss\n",
    "        dscores = probs.copy()\n",
    "        dscores[np.arange(N), y] -= 1  # Subtract 1 from correct class\n",
    "        \n",
    "        # Compute gradients with respect to W and b\n",
    "        dW = x.T @ dscores\n",
    "        db = np.sum(dscores, axis=0)\n",
    "        \n",
    "        # Update parameters using gradient descent\n",
    "        self.W -= self.lr * dW\n",
    "        self.b -= self.lr * db\n",
    "        \n",
    "        return dW, db\n",
    "    \n",
    "    def sgd(self, x, y, epochs=10, batch_size=128):\n",
    "        N = len(y)\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, N, batch_size):\n",
    "                x_batch = x[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "                scores = self.forward(x_batch)\n",
    "                probs = self.softmax(scores)\n",
    "                loss = self.crossEntropyLoss(probs, y_batch)\n",
    "                self.backward(x_batch, y_batch, probs)\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Epoch {epoch+1:2.0f}, Loss: {loss:.3f}\")\n",
    "            \n",
    "        return self.W, self.b\n",
    "\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        predictions, _ = self.predict(X)\n",
    "        return np.mean(predictions == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.268\n",
      "Train Accuracy: 90.05%\n",
      "Test Accuracy : 89.33%\n"
     ]
    }
   ],
   "source": [
    "model = LinearClassifier()\n",
    "w, b = model.sgd(x_train, y_train, batch_size=128)\n",
    "\n",
    "train_accuracy = model.accuracy(x_train, y_train)\n",
    "print(f\"Train Accuracy: {train_accuracy*100:3.2f}%\")\n",
    "\n",
    "test_accuracy = model.accuracy(x_test, y_test)\n",
    "print(f\"Test Accuracy : {test_accuracy*100:3.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
