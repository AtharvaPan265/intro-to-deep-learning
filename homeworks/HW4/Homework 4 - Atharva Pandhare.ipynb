{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6e6936",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9793e500",
   "metadata": {},
   "source": [
    "## A. Optimization Comparisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ac6cb",
   "metadata": {},
   "source": [
    "### Comparing SGD, SGD+Momentum, AdaGrad, RMSprop:\n",
    "- Conventional Standard Gradient Descent (SGD)\n",
    "    - Simplest method\n",
    "    - Updates are solely based on learning rate and the current batch's gradient\n",
    "    - ***Not Perfect:*** Gradient can get stuck in a local minima or a saddle point\n",
    "    $$x \\mathrel{+}= -lr * dx$$\n",
    "- Standard Gradient Descent with **Momentum** (SGD+Momentum)\n",
    "    - Adds a momentum component which can be used to carry some of the velocity of the weights\n",
    "    - Velocity a decaying accumulation of the gradients\n",
    "    - Aims to solve issue from SGD of getting stuck in minima or saddle \n",
    "    - Momentum aspect accelerate convergence, and dampens oscillations.\n",
    "    $$v = \\mu * v - lr * dx\\\\ x \\mathrel{+}= v$$\n",
    "    - adds v which is the velocity\n",
    "    - adds $\\mu$ which is the momentum normally set to $\\mathrel{\\approx}0.9$ \n",
    "- Adaptive Gradient (AdaGrad)\n",
    "    - Element-wise adaptively adjust effective learning rate\n",
    "        - for weights with high gradients: reduce\n",
    "        - for weights with small/infrequent updates: increase\n",
    "    $$cache \\mathrel{+}= dx^2 \\\\ x \\mathrel{+}= \\frac{-lr * dx}{\\sqrt{cache}+\\epsilon}$$\n",
    "    - $1\\mathrm{e}{-8}<\\epsilon<1\\mathrm{e}{-4}$\n",
    "    - $cache$: sum of squared gradients    \n",
    "    - ***Not Perfect***: Cache increases causeing the effective learning rate to decreases, potentially causing learningto stop early\n",
    "- Root Mean Squared Propagation  (RMSprop)\n",
    "    - Builds on AdaGrad\n",
    "    - Aims to solve gripes of AdaGrad by using an exponetially decaying moving average $cache$\n",
    "    $$cache = decay * cache + (1-decay)*dx^2 \\\\  x \\mathrel{+}= \\frac{-lr * dx}{\\sqrt{cache}+\\epsilon} $$\n",
    "    - $decay$ is typically $0.9, 0.99, 0.999 ...$\n",
    "    - Prevents the learning rate from vanishing too quickly, allowing for continued learning\n",
    "\n",
    "### Popularity of Adam\n",
    "- Adam's popularity comes because it does the adaptive learning rate from RMSprop and also brings in the momentum aspect from SGD+Momentum.\n",
    "- These features allow adam to be able to navigate complex loss landscapes\n",
    "- It converges faster, being more efficient and compute friendly\n",
    "- These factors and the effectiveness across various tasks such as NLP, and computer vision has led to it wide spread adoption in the field.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19f1409",
   "metadata": {},
   "source": [
    "## B. Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22e4577",
   "metadata": {},
   "source": [
    "### Imports and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191cf6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in relevant libraries, and alias where appropriate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Define relevant variables for the ML task\n",
    "batch_size = 256\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10 # 20 just gets almost 100 everytime\n",
    "\n",
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# For training data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST standard mean and standard deviation\n",
    "])\n",
    "\n",
    "cifar_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# For test data\n",
    "cifar_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "# DataLoader for training and test datasets\n",
    "trainloader = torch.utils.data.DataLoader(cifar_trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(cifar_testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d34b20",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 10)\n",
    "            # nn.Softmax(dim=1) # Not needed for CrossEntropyLoss\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = MLP().net.to(device)\n",
    "criterion = nn.CrossEntropyLoss() # Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94324de4",
   "metadata": {},
   "source": [
    "### Train and Test Funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b3fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(optimizer, name = 'err', scheduler=None, model=model, trainloader=trainloader, num_epochs=num_epochs):\n",
    "    start_time = time.time()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Lists to store losses for plotting\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        batches_in_epoch = 0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            # Move data to device (CPU/GPU)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate statistics\n",
    "            running_loss += loss.item()\n",
    "            batches_in_epoch += 1\n",
    "        \n",
    "        # Step the scheduler if it exists\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Store average loss for this epoch\n",
    "        avg_epoch_loss = running_loss / batches_in_epoch\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        print(f'Epoch {epoch + 1}, Loss: {avg_epoch_loss:.3f}')\n",
    "    \n",
    "    # Plot the training loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o')\n",
    "    plt.title('Training Loss Over Epochs for MLP' + name)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training completed in {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "\n",
    "def test(model = model, testloader=testloader):\n",
    "    start_time = time.time()\n",
    "    # Testing the best model on test data\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f'testing finished in {elapsed_time:.2f} seconds, Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "def train_test(optimizer:torch.optim, name = 'err', scheduler=None):\n",
    "    train(optimizer=optimizer, name=name, scheduler=scheduler)\n",
    "    return test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bffad4",
   "metadata": {},
   "source": [
    "### Implementations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08028dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = {\n",
    "    'sgd': torch.optim.SGD(model.parameters(), lr=learning_rate),\n",
    "    'sgd_momentum': torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9),\n",
    "    'adagrad': torch.optim.Adagrad(model.parameters(), lr=learning_rate, eps=1e-6),\n",
    "    'rmsprop': torch.optim.RMSprop(model.parameters(), lr=learning_rate, alpha=0.9, eps=1e-6),\n",
    "    'adam': torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "}\n",
    "results = {}\n",
    "def try_all(optimizers = optimizers):\n",
    "    for optimizer in optimizers:\n",
    "        optim = optimizers[optimizer]\n",
    "        print_out = f\"Training with optimiser: {optimizer}, lr_scheduler: None\"\n",
    "        print(print_out)\n",
    "        results[print_out] = train_test(optim, name=print_out)\n",
    "        print_out = f\"Training with optimiser: {optimizer}, lr_scheduler: StepLR\"\n",
    "        print(print_out)\n",
    "        results[print_out] = train_test(optim, name=print_out, scheduler=torch.optim.lr_scheduler.StepLR(optimizers[optimizer], step_size=2, gamma=0.1))\n",
    "        print_out = f\"Training with optimiser: {optimizer}, lr_scheduler: CosineAnnealingLR\"\n",
    "        print(print_out)\n",
    "        results[print_out] = train_test(optim, name=print_out, scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(optimizers[optimizer], T_max=10))\n",
    "                \n",
    "\n",
    "try_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c48f2da",
   "metadata": {},
   "source": [
    "### At the end of it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de02ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    print(f\"After {result}, Accuracy: {results[result]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c072f4",
   "metadata": {},
   "source": [
    "### Write-up\n",
    "I wrote a loop to run through all the models, and per model it runs every learning rate scheduler.\n",
    "I tried a few different values for num_epochs, I found that using 20 epochs always ended up giving really high accuracy so I was not able to tell a difference. I stuck with 10 to create more of a difference.\n",
    "\n",
    "#### Findings\n",
    "Comparing the optimizers I found that SGD (Stochastic Gradient Descent) performed relatively poorly with an accuracy of around 93-94%. Adding momentum to SGD (SGD with momentum) significantly improved the results, reaching around 97-98% accuracy. Adagrad did even better, consistently achieving about 98.13% accuracy. RMSProp and Adam fell somewhere in between, with accuracies around 95-98%.\n",
    "\n",
    "Comparing the learning rate schedulers, StepLR and CosineAnnealingLR generally helped improve or maintain the accuracy compared to having no scheduler. However, with optimizers like Adagrad, the scheduler didn't seem to make a difference. Overall, using SGD with momentum or Adagrad seemed to yield the best results, especially when combined with a scheduler like StepLR or CosineAnnealingLR.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd44243",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd6d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define the target function as the sum of 3 sinusoids\n",
    "def target_function(x):\n",
    "    return np.sin(2 * np.pi * 1.0 * x) + 0.5 * np.sin(2 * np.pi * 3.0 * x) + 0.25 * np.sin(2 * np.pi * 5.0 * x)\n",
    "\n",
    "# Generate dataset with 4000 points\n",
    "x = np.linspace(0, 1, 4000)  # 4000 points between 0 and 1\n",
    "y = target_function(x)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_train = torch.tensor(x, dtype=torch.float32).view(-1, 1)\n",
    "y_train = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Define the original 3-layer MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, 1)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Define the LFF (Learnable Fourier Feature) layer\n",
    "class LFF(nn.Module):\n",
    "    def __init__(self, in_features, out_features, scale=1.0, init=\"iso\", sincos=False):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.sincos = sincos\n",
    "        self.out_features = out_features\n",
    "        self.scale = scale\n",
    "        if self.sincos:\n",
    "            self.linear = nn.Linear(in_features, out_features // 2)\n",
    "        else:\n",
    "            self.linear = nn.Linear(in_features, out_features)\n",
    "        if init == \"iso\":\n",
    "            nn.init.normal_(self.linear.weight, 0, scale / in_features)\n",
    "            nn.init.normal_(self.linear.bias, 0, 1)\n",
    "        else:\n",
    "            nn.init.uniform_(self.linear.weight, -scale / in_features, scale / in_features)\n",
    "            nn.init.uniform_(self.linear.bias, -1, 1)\n",
    "        if self.sincos:\n",
    "            nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = np.pi * self.linear(x)\n",
    "        if self.sincos:\n",
    "            return torch.cat([torch.sin(x), torch.cos(x)], dim=-1)\n",
    "        else:\n",
    "            return torch.sin(x)\n",
    "\n",
    "# Define the FourierModel with the LFF layer\n",
    "class FourierModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(FourierModel, self).__init__()\n",
    "        self.input_layer = LFF(state_dim, hidden_dim, scale=0.1, init=\"iso\", sincos=False)\n",
    "        self.mid_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.output = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.mid_layer(x)\n",
    "        x = self.relu2(x)\n",
    "        return self.output(x)\n",
    "\n",
    "# Initialize models, loss function, and optimizers\n",
    "mlp_model = MLP()\n",
    "fourier_model = FourierModel(state_dim=1, action_dim=1, hidden_dim=100)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=0.002)\n",
    "fourier_optimizer = optim.Adam(fourier_model.parameters(), lr=0.002)\n",
    "\n",
    "# Training loop for both models\n",
    "num_epochs = 4000\n",
    "mlp_losses = []\n",
    "fourier_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train MLP model\n",
    "    mlp_model.train()\n",
    "    mlp_optimizer.zero_grad()\n",
    "    mlp_y_pred = mlp_model(x_train)\n",
    "    mlp_loss = criterion(mlp_y_pred, y_train)\n",
    "    mlp_loss.backward()\n",
    "    mlp_optimizer.step()\n",
    "    mlp_losses.append(mlp_loss.item())\n",
    "\n",
    "    # Train Fourier model\n",
    "    fourier_model.train()\n",
    "    fourier_optimizer.zero_grad()\n",
    "    fourier_y_pred = fourier_model(x_train)\n",
    "    fourier_loss = criterion(fourier_y_pred, y_train)\n",
    "    fourier_loss.backward()\n",
    "    fourier_optimizer.step()\n",
    "    fourier_losses.append(fourier_loss.item())\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], MLP Loss: {mlp_loss.item():.4f}, Fourier Loss: {fourier_loss.item():.4f}\")\n",
    "\n",
    "# Plot the training loss for both models\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(mlp_losses, label=\"MLP Loss\")\n",
    "plt.plot(fourier_losses, label=\"Fourier Model Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the target function vs. the learned functions\n",
    "with torch.no_grad():\n",
    "    mlp_y_learned = mlp_model(x_train).numpy()\n",
    "    fourier_y_learned = fourier_model(x_train).numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, y, label=\"Target Function\", color='blue')\n",
    "plt.plot(x, mlp_y_learned, label=\"MLP Learned Function\", color='red', linestyle='--')\n",
    "plt.plot(x, fourier_y_learned, label=\"Fourier Model Learned Function\", color='green', linestyle='--')\n",
    "plt.title(\"Target Function vs MLP and Fourier Model Learned Functions\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54acbb69",
   "metadata": {},
   "source": [
    "- What is the input and output of the model? What does this code do?\n",
    "    - **MLP Model**      \n",
    "        - **Input:** Takes in a sample of shape: 1\n",
    "        - **Output:** ouptuts a sample of shape: 1\n",
    "    - **Fourier Loss**     \n",
    "        - **Input:** Takes in a sample of shape: 1    \n",
    "        - **Output:** ouptuts a sample of shape: 1     \n",
    "    The code trains the aforementioned neural networks to approximate a target function. it then compares their losses, then compares the learned functions to the target function.\n",
    "\n",
    "- What a role does the LFF layer shown in the code play? Explain briefly.    \n",
    "    The LFF (Learnable Fourier Feature) layer transforms the input into a higher-dimensional space using sine functions (since sincos=False). This transformation helps the model represnt the sample's in higher dimensionality, this allows the model to train on the frequencies and amplitudes generated. Here `sincos` is set to `False` without this it would generate both $[sin(W_n X_n), cos(W_n x_n)]$, using only $sin(W_n x_n)$ is beneficial for simplicity and can be effective for capturing periodic patterns in the dataset.\n",
    "\n",
    "- Run the code and describe what you observe. How do the results relate to the Universal Function Approximation Theorem of neural networks?    \n",
    "    **Observations**    \n",
    "    We can observe that both the learned functions are very similar to the target function. Although the LFF funciton is much closer to the target funciton. This behavior might be because of the fourier features being in a higher dimentionality, allowing for better feature detection for samples with lower complexities.    \n",
    "    **universal approximation theorem states**     \n",
    "    The *universal approximation theorem states* that any continuous function $f$ can be approximated arbitrarily well by a neural network with at least 1 hidden layer with a finite number of weights [[source]](https://mitliagkas.github.io/ift6085-2020/ift-6085-lecture-10-notes.pdf).    \n",
    "    **How Observations relate to UAT**  \n",
    "    The similarity between the observations and the UAT demonstrate the theorem in practice. increaseing the number of weights and representing the samples in a higher dimentionality can prove to be effective for funciton approximators.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
